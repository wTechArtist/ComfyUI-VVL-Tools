import json
import re
import ast
from comfy.comfy_types.node_typing import IO


class JsonObjectDeduplicator:
    """
    Remove duplicate objects from JSON based on name (excluding numeric suffix) and scale.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING, IO.STRING)
    RETURN_NAMES = ("deduplicated_json", "removed_duplicates")
    FUNCTION = "deduplicate_objects"
    CATEGORY = "VVL/json"
    
    def deduplicate_objects(self, json_text, **kwargs):
        try:
            # Parse input JSON
            data = json.loads(json_text)
            
            # Check if 'objects' field exists
            if 'objects' not in data:
                return json_text, json.dumps({"removed_objects": []})
            
            objects = data['objects']
            unique_objects = []
            removed_objects = []
            kept_original_indices = []
            
            # Track unique combinations of (base_name, scale)
            seen_combinations = {}
            
            for i, obj in enumerate(objects):
                # Extract base name (remove numeric suffix)
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                
                # Get scale as tuple for comparison
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                # Create combination key
                combination_key = (base_name, scale_tuple)
                
                if combination_key not in seen_combinations:
                    # First occurrence - keep it
                    seen_combinations[combination_key] = len(unique_objects)
                    unique_objects.append(obj)
                    kept_original_indices.append(i)
                else:
                    # Duplicate - add to removed list
                    removed_obj = obj.copy()
                    removed_obj['_original_index'] = i
                    removed_obj['_duplicate_of_index'] = seen_combinations[combination_key]
                    removed_objects.append(removed_obj)
            
            # Update data with deduplicated objects
            data['objects'] = unique_objects
            
            # Create result JSONs
            deduplicated_json = json.dumps(data, ensure_ascii=False, indent=2)
            removed_duplicates = json.dumps({
                "removed_objects": removed_objects,
                "original_count": len(objects),
                "deduplicated_count": len(unique_objects),
                "kept_original_indices": kept_original_indices
            }, ensure_ascii=False, indent=2)
            
            return deduplicated_json, removed_duplicates
            
        except json.JSONDecodeError as e:
            error_msg = f"Invalid JSON input: {str(e)}"
            return json.dumps({"error": error_msg}), json.dumps({"error": error_msg})
        except Exception as e:
            error_msg = f"Error processing JSON: {str(e)}"
            return json.dumps({"error": error_msg}), json.dumps({"error": error_msg})


class JsonObjectMerger:
    """
    Merge processed JSON with removed duplicates, using the same 3d_url for objects with same name and scale.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "processed_json": (IO.STRING, {"multiline": True, "default": ""}),
                "removed_duplicates": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("merged_json",)
    FUNCTION = "merge_objects"
    CATEGORY = "VVL/json"
    
    def merge_objects(self, processed_json, removed_duplicates, **kwargs):
        try:
            # Parse input JSONs
            processed_data = json.loads(processed_json)
            removed_data = json.loads(removed_duplicates)
            
            # Check for errors in input
            if "error" in processed_data or "error" in removed_data:
                return (json.dumps({"error": "Input contains errors"}),)
            
            # Check if required fields exist
            if 'objects' not in processed_data or 'removed_objects' not in removed_data:
                return (processed_json,)
            
            processed_objects = processed_data['objects']
            removed_objects = removed_data['removed_objects']
            kept_original_indices = removed_data.get('kept_original_indices', [])
            original_count = removed_data.get('original_count', len(processed_objects) + len(removed_objects))
            
            # Create a mapping of (base_name, scale) to 3d_url from processed objects
            url_mapping = {}
            for obj in processed_objects:
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                combination_key = (base_name, scale_tuple)
                if '3d_url' in obj:
                    url_mapping[combination_key] = obj['3d_url']
            
            # Restore removed objects with correct 3d_url
            restored_objects = []
            for removed_obj in removed_objects:
                # Remove metadata fields
                obj = {k: v for k, v in removed_obj.items() 
                      if not k.startswith('_')}
                
                # Find matching 3d_url
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                combination_key = (base_name, scale_tuple)
                if combination_key in url_mapping:
                    obj['3d_url'] = url_mapping[combination_key]
                
                restored_objects.append(obj)
            
            # Reconstruct original order using kept_original_indices and _original_index
            full_objects = [None] * max(original_count, len(processed_objects) + len(restored_objects))

            # Place processed (kept) objects back to their original indices
            for idx, obj in enumerate(processed_objects):
                target_index = kept_original_indices[idx] if idx < len(kept_original_indices) else idx
                if 0 <= target_index < len(full_objects):
                    full_objects[target_index] = obj
                else:
                    # Fallback append if index out of bounds
                    full_objects.append(obj)

            # Place restored (previously removed) objects at their original indices
            for removed_obj, restored in zip(removed_objects, restored_objects):
                target_index = removed_obj.get('_original_index')
                if isinstance(target_index, int) and 0 <= target_index < len(full_objects):
                    full_objects[target_index] = restored
                else:
                    full_objects.append(restored)

            # Remove any None slots while preserving order
            all_objects = [obj for obj in full_objects if obj is not None]

            # Update data with all objects
            processed_data['objects'] = all_objects
            
            # Create final JSON
            merged_json = json.dumps(processed_data, ensure_ascii=False, indent=2)
            
            return (merged_json,)
            
        except json.JSONDecodeError as e:
            error_msg = f"Invalid JSON input: {str(e)}"
            return (json.dumps({"error": error_msg}),)
        except Exception as e:
            error_msg = f"Error merging JSON: {str(e)}"
            return (json.dumps({"error": error_msg}),)


# Extract subject, names list, and scales list from JSON
class JsonExtractSubjectNamesScales:
    """
    Extracts subject, names list, and scales list from the input JSON.

    - names_json: JSON string array of names, optionally stripping numeric suffixes
    - scales_json: JSON string array of scale triplets (e.g., [[0.7,0.5,0.75], ...])
    Returning JSON strings keeps strong compatibility with community nodes.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""}),
                "strip_numeric_suffix": (IO.BOOLEAN, {"default": True, "label_on": "strip", "label_off": "keep"}),
            }
        }

    RETURN_TYPES = (IO.STRING, IO.ANY, IO.ANY)
    RETURN_NAMES = ("subject", "names_list", "scales_list")
    FUNCTION = "extract_fields"
    CATEGORY = "VVL/json"

    def extract_fields(self, json_text, strip_numeric_suffix=True, **kwargs):
        try:
            data = json.loads(json_text)
            subject = data.get("subject", "")
            objects = data.get("objects", [])

            names = []
            scales = []
            for obj in objects:
                name = obj.get("name", "")
                if strip_numeric_suffix:
                    name = re.sub(r"\d+$", "", name).strip()
                names.append(name)
                scales.append(obj.get("scale", []))

            return (
                subject,
                names,
                scales,
            )
        except json.JSONDecodeError as e:
            msg = json.dumps({"error": f"Invalid JSON input: {str(e)}"})
            return (msg, msg, msg)
        except Exception as e:
            msg = json.dumps({"error": f"Error extracting fields: {str(e)}"})
            return (msg, msg, msg)


class ApplyUrlsToJson:
    """
    Apply a list of (index, url) pairs to deduplicated_json.objects[index].3d_url.

    - Robust to inputs shaped like [[idx, url], ...]
    - Skips entries with None url (configurable)
    - Ignores out-of-bounds indices (configurable)
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "deduplicated_json": (IO.STRING, {"multiline": True, "default": ""}),
                "idx_url_list": (IO.ANY, {}),
            },
            "optional": {
                "field_name": (IO.STRING, {"default": "3d_url"}),
                "skip_none_url": (IO.BOOLEAN, {"default": True, "label_on": "skip", "label_off": "write None"}),
                "ignore_oob_index": (IO.BOOLEAN, {"default": True, "label_on": "ignore", "label_off": "error"}),
            },
        }

    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("processed_json",)
    FUNCTION = "apply"
    CATEGORY = "VVL/json"

    def _normalize_pairs(self, idx_url_list):
        # Debug: print what we actually received
        print(f"ApplyUrlsToJson received: {type(idx_url_list)} = {idx_url_list}")
        
        pairs = []
        if idx_url_list is None:
            return pairs

        # Handle various input formats from different ComfyUI node outputs
        container = idx_url_list
        if not isinstance(container, (list, tuple)):
            container = [container]

        # Special case: check if container itself is a single [idx, url] pair
        if (isinstance(container, (list, tuple)) and len(container) == 2 and
            not isinstance(container[0], (list, tuple, str)) and  # first element is not nested
            isinstance(container[1], str)):  # second element is string (url)
            try:
                idx = int(container[0])
                url = container[1]
                pairs.append((idx, url))
                print(f"ApplyUrlsToJson detected single pair: [{idx}, '{url}']")
                print(f"ApplyUrlsToJson normalized to: {pairs}")
                return pairs
            except (ValueError, TypeError):
                # If conversion fails, fall through to normal processing
                pass

        for item in container:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                # Direct [idx, url] pair
                pairs.append((item[0], item[1]))
            elif isinstance(item, str) and "," in item:
                # CR_IntertwineLists style: "idx, url"
                parts = item.split(",", 1)
                if len(parts) == 2:
                    try:
                        idx = int(parts[0].strip())
                        url = parts[1].strip()
                        pairs.append((idx, url))
                    except ValueError:
                        continue
            elif hasattr(item, '__iter__') and not isinstance(item, str):
                # Nested containers - recursive flatten
                for subitem in item:
                    if isinstance(subitem, (list, tuple)) and len(subitem) >= 2:
                        pairs.append((subitem[0], subitem[1]))
        
        print(f"ApplyUrlsToJson normalized to: {pairs}")
        return pairs

    def apply(self, deduplicated_json, idx_url_list, field_name="3d_url", skip_none_url=True, ignore_oob_index=True, **kwargs):
        try:
            data = json.loads(deduplicated_json)
        except json.JSONDecodeError as e:
            return (json.dumps({"error": f"Invalid JSON input: {str(e)}"}),)

        if not isinstance(data, dict) or "objects" not in data or not isinstance(data["objects"], list):
            return (deduplicated_json,)

        objects = data["objects"]

        pairs = self._normalize_pairs(idx_url_list)
        for raw_idx, url in pairs:
            try:
                idx = int(raw_idx)
            except Exception:
                continue

            if idx < 0 or idx >= len(objects):
                if ignore_oob_index:
                    continue
                else:
                    return (json.dumps({"error": f"Index out of bounds: {idx}"}, ensure_ascii=False),)

            if url is None and skip_none_url:
                continue

            obj = objects[idx]
            if isinstance(obj, dict):
                obj[field_name] = url

        processed_json = json.dumps(data, ensure_ascii=False, indent=2)
        return (processed_json,)




class JsonMarkdownCleaner:
    """
    æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„ Markdown ä»£ç å—æ ‡è®°ã€‚
    
    ç§»é™¤ï¼š
    - å¼€å¤´çš„ ```json æˆ– ```
    - ç»“å°¾çš„ ```
    - ä¿æŒ JSON å†…å®¹å®Œæ•´
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("cleaned_json",)
    FUNCTION = "clean_json"
    CATEGORY = "VVL/json"
    
    def clean_json(self, json_text, **kwargs):
        """æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„ Markdown ä»£ç å—æ ‡è®°"""
        try:
            cleaned_text = json_text.strip()
            
            # ç§»é™¤å¼€å¤´çš„ ```json æˆ– ```
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:].strip()
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:].strip()
            
            # ç§»é™¤ç»“å°¾çš„ ```
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3].strip()
            
            # éªŒè¯æ¸…ç†åçš„æ–‡æœ¬æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
            try:
                json.loads(cleaned_text)
            except json.JSONDecodeError as e:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œè¿”å›è­¦å‘Šä½†ä¸é˜»æ­¢å¤„ç†
                print(f"Warning: Cleaned text may not be valid JSON: {str(e)}")
            
            return (cleaned_text,)
            
        except Exception as e:
            error_msg = f"æ¸…ç† JSON æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonMarkdownCleaner error: {error_msg}")
            return (json_text,)  # å‡ºé”™æ—¶è¿”å›åŸå§‹æ–‡æœ¬


class IndexUrlPairDeduplicator:
    """
    ç§»é™¤ (index, url) å¯¹åˆ—è¡¨ä¸­çš„é‡å¤é¡¹ã€‚
    
    æ”¯æŒä¸‰ç§ç­–ç•¥ï¼š
    - remove_all: å®Œå…¨åˆ é™¤æ‰€æœ‰é‡å¤é¡¹ï¼ˆåŒ…æ‹¬ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
    - keep_first: ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„é‡å¤é¡¹
    - keep_last: ä¿ç•™æœ€åä¸€ä¸ªå‡ºç°çš„é‡å¤é¡¹
    
    æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œå¦‚ [[idx, url], ...] æˆ– ["idx,url", ...]
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "idx_url_list": (IO.ANY, {}),
            },
            "optional": {
                "preserve_order": (IO.BOOLEAN, {"default": True, "label_on": "ä¿æŒé¡ºåº", "label_off": "ä¸ä¿è¯é¡ºåº"}),
                "duplicate_strategy": (["keep_first", "keep_last", "remove_all"], {"default": "remove_all"}),
            },
        }
    
    RETURN_TYPES = (IO.ANY,)
    RETURN_NAMES = ("deduplicated_pairs",)
    FUNCTION = "deduplicate_pairs"
    CATEGORY = "VVL/json"
    
    def _normalize_pairs(self, idx_url_list):
        """è§„èŒƒåŒ–è¾“å…¥æ•°æ®ä¸º (index, url) å¯¹çš„åˆ—è¡¨"""
        pairs = []
        if idx_url_list is None:
            return pairs

        container = idx_url_list
        if not isinstance(container, (list, tuple)):
            container = [container]

        for item in container:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                # ç›´æ¥çš„ [idx, url] å¯¹
                pairs.append((item[0], item[1]))
            elif isinstance(item, str) and "," in item:
                # CR_IntertwineLists é£æ ¼: "idx, url"
                parts = item.split(",", 1)
                if len(parts) == 2:
                    try:
                        idx = int(parts[0].strip())
                        url = parts[1].strip()
                        pairs.append((idx, url))
                    except ValueError:
                        continue
            elif hasattr(item, '__iter__') and not isinstance(item, str):
                # åµŒå¥—å®¹å™¨ - é€’å½’å±•å¼€
                for subitem in item:
                    if isinstance(subitem, (list, tuple)) and len(subitem) >= 2:
                        pairs.append((subitem[0], subitem[1]))
        
        return pairs
    
    def deduplicate_pairs(self, idx_url_list, preserve_order=True, duplicate_strategy="remove_all", **kwargs):
        """å»é™¤é‡å¤çš„ (index, url) å¯¹"""
        try:
            pairs = self._normalize_pairs(idx_url_list)
            
            if not pairs:
                return (idx_url_list,)
            
            # ç»Ÿè®¡æ¯ä¸ªé”®å‡ºç°çš„æ¬¡æ•°
            key_counts = {}
            for idx, url in pairs:
                key = (idx, url)
                key_counts[key] = key_counts.get(key, 0) + 1
            
            if duplicate_strategy == "remove_all":
                # ç§»é™¤æ‰€æœ‰é‡å¤é¡¹ï¼ˆåŒ…æ‹¬ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
                if preserve_order:
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        # åªä¿ç•™å‡ºç°æ¬¡æ•°ä¸º1çš„é¡¹
                        if key_counts[key] == 1:
                            deduplicated.append([idx, url])
                else:
                    # ä¸ä¿è¯é¡ºåºï¼Œç›´æ¥è¿‡æ»¤
                    unique_keys = {k: [k[0], k[1]] for k, count in key_counts.items() if count == 1}
                    deduplicated = list(unique_keys.values())
                    
            elif duplicate_strategy == "keep_first":
                # ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„
                if preserve_order:
                    seen = set()
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        if key not in seen:
                            seen.add(key)
                            deduplicated.append([idx, url])
                else:
                    seen = {}
                    for idx, url in pairs:
                        key = (idx, url)
                        if key not in seen:
                            seen[key] = [idx, url]
                    deduplicated = list(seen.values())
                    
            elif duplicate_strategy == "keep_last":
                # ä¿ç•™æœ€åä¸€ä¸ªå‡ºç°çš„
                if preserve_order:
                    seen = set()
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        if key in seen:
                            # ç§»é™¤ä¹‹å‰æ·»åŠ çš„ç›¸åŒé¡¹
                            deduplicated = [pair for pair in deduplicated 
                                          if (pair[0], pair[1]) != key]
                        seen.add(key)
                        deduplicated.append([idx, url])
                else:
                    # ä¸ä¿è¯é¡ºåºï¼Œä¿ç•™æœ€åä¸€ä¸ª
                    seen = {}
                    for idx, url in pairs:
                        key = (idx, url)
                        seen[key] = [idx, url]
                    deduplicated = list(seen.values())
            
            print(f"IndexUrlPairDeduplicator: è¾“å…¥ {len(pairs)} å¯¹ï¼Œè¾“å‡º {len(deduplicated)} å¯¹")
            print(f"ç­–ç•¥: {duplicate_strategy}")
            print(f"å»é‡å‰: {pairs}")
            print(f"å»é‡å: {deduplicated}")
            
            # è¾“å‡ºé‡å¤é¡¹ç»Ÿè®¡ä¿¡æ¯
            duplicates = {k: count for k, count in key_counts.items() if count > 1}
            if duplicates:
                print(f"å‘ç°çš„é‡å¤é¡¹: {duplicates}")
            
            return (deduplicated,)
            
        except Exception as e:
            error_msg = f"å»é‡å¤„ç†é”™è¯¯: {str(e)}"
            print(f"IndexUrlPairDeduplicator error: {error_msg}")
            return (idx_url_list,)


class JsonArrayElementFieldExtractor:
    """
    ä»JSONæ•°ç»„ä¸­æå–æŒ‡å®šç´¢å¼•å…ƒç´ çš„æŒ‡å®šå­—æ®µå€¼ã€‚
    
    ğŸ“‹ æ”¯æŒè¾“å…¥æ ¼å¼ï¼š
    â€¢ JSONå­—ç¬¦ä¸²æ•°ç»„: '[{"name":"file1","url":"path1"}, ...]'
    â€¢ Pythonåˆ—è¡¨å¯¹è±¡: [{"name":"file1","url":"path1"}, ...]
    
    ğŸ¯ ç´¢å¼•ç”¨æ³•è¯¦è§£ï¼š
    â€¢ æ­£æ•°ç´¢å¼•: ä»æ•°ç»„å¼€å¤´è®¡æ•°
      - 0 = ç¬¬1ä¸ªå…ƒç´ 
      - 1 = ç¬¬2ä¸ªå…ƒç´   
      - 2 = ç¬¬3ä¸ªå…ƒç´  ...
    
    â€¢ è´Ÿæ•°ç´¢å¼•: ä»æ•°ç»„æœ«å°¾è®¡æ•°
      - -1 = æœ€å1ä¸ªå…ƒç´ 
      - -2 = å€’æ•°ç¬¬2ä¸ªå…ƒç´ 
      - -3 = å€’æ•°ç¬¬3ä¸ªå…ƒç´  ...
    
    ğŸ’¡ å®ç”¨ç¤ºä¾‹ï¼š
    æ•°ç»„: ["A", "B", "C", "D", "E"]
    ç´¢å¼•å¯¹ç…§è¡¨:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æ­£ç´¢å¼•  â”‚    0    â”‚    1    â”‚    2    â”‚    3    â”‚    4    â”‚
    â”‚ å…ƒç´ å€¼  â”‚   "A"   â”‚   "B"   â”‚   "C"   â”‚   "D"   â”‚   "E"   â”‚
    â”‚ è´Ÿç´¢å¼•  â”‚   -5    â”‚   -4    â”‚   -3    â”‚   -2    â”‚   -1    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    âš™ï¸ å¯é…ç½®å‚æ•°ï¼š
    â€¢ index: æ•°ç»„ç´¢å¼•ä½ç½®ï¼ˆæ”¯æŒæ­£è´Ÿæ•°ï¼‰
    â€¢ field_name: è¦æå–çš„å­—æ®µåç§°
    â€¢ return_empty_on_error: é”™è¯¯å¤„ç†æ–¹å¼
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_array": (IO.ANY, {}),
            },
            "optional": {
                "index": (IO.INT, {"default": 0, "min": -999999, "max": 999999, "tooltip": "æ•°ç»„ç´¢å¼•ä½ç½®\nâ€¢ æ­£æ•°: ä»å¤´å¼€å§‹ (0=ç¬¬1ä¸ª, 1=ç¬¬2ä¸ª, ...)\nâ€¢ è´Ÿæ•°: ä»å°¾å¼€å§‹ (-1=æœ€å1ä¸ª, -2=å€’æ•°ç¬¬2ä¸ª, ...)\nâ€¢ ç¤ºä¾‹: æœ‰5ä¸ªå…ƒç´ çš„æ•°ç»„\n  ç´¢å¼• 0,1,2,3,4 å¯¹åº” ç¬¬1,2,3,4,5ä¸ªå…ƒç´ \n  ç´¢å¼• -1,-2,-3,-4,-5 å¯¹åº” æœ€å1,2,3,4,5ä¸ªå…ƒç´ "}),
                "field_name": (IO.STRING, {"default": "url", "tooltip": "è¦æå–çš„å­—æ®µåç§°\nâ€¢ é»˜è®¤: 'url'\nâ€¢ å¸¸ç”¨å­—æ®µ: name, cover, id, path ç­‰\nâ€¢ æ”¯æŒä»»æ„å¯¹è±¡å±æ€§å"}),
                "return_empty_on_error": (IO.BOOLEAN, {"default": True, "label_on": "è¿”å›ç©ºå€¼", "label_off": "è¿”å›é”™è¯¯ä¿¡æ¯", "tooltip": "å‡ºé”™æ—¶çš„å¤„ç†æ–¹å¼\nâ€¢ å¼€å¯: å‡ºé”™è¿”å›ç©ºå­—ç¬¦ä¸² ''\nâ€¢ å…³é—­: å‡ºé”™è¿”å›é”™è¯¯ä¿¡æ¯æè¿°\næ¨èå¼€å¯ä»¥ä¿æŒå·¥ä½œæµç¨³å®šæ€§"}),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("field_value",)
    FUNCTION = "extract_field_value"
    CATEGORY = "VVL/json"
    
    def extract_field_value(self, json_array, index=0, field_name="url", return_empty_on_error=True, **kwargs):
        """ä»æ•°ç»„ä¸­æå–æŒ‡å®šç´¢å¼•å…ƒç´ çš„å­—æ®µå€¼"""
        try:
            # å¤„ç†è¾“å…¥æ•°æ®
            data = json_array
            
            # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            if isinstance(json_array, str):
                try:
                    data = json.loads(json_array)
                except json.JSONDecodeError:
                    if return_empty_on_error:
                        return ("",)
                    else:
                        return (f"é”™è¯¯: æ— æ•ˆçš„JSONå­—ç¬¦ä¸²",)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨/æ•°ç»„
            if not isinstance(data, list):
                if return_empty_on_error:
                    return ("",)
                else:
                    return (f"é”™è¯¯: è¾“å…¥ä¸æ˜¯æ•°ç»„æ ¼å¼",)
            
            # æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º
            if len(data) == 0:
                if return_empty_on_error:
                    return ("",)
                else:
                    return (f"é”™è¯¯: æ•°ç»„ä¸ºç©º",)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            try:
                # Pythonæ”¯æŒè´Ÿæ•°ç´¢å¼•ï¼Œä½†æˆ‘ä»¬éœ€è¦æ£€æŸ¥è¾¹ç•Œ
                if index >= len(data) or index < -len(data):
                    if return_empty_on_error:
                        return ("",)
                    else:
                        return (f"é”™è¯¯: ç´¢å¼• {index} è¶…å‡ºæ•°ç»„èŒƒå›´ (0 åˆ° {len(data)-1})",)
                
                # è·å–æŒ‡å®šç´¢å¼•çš„å…ƒç´ 
                target_element = data[index]
                
            except IndexError:
                if return_empty_on_error:
                    return ("",)
                else:
                    return (f"é”™è¯¯: ç´¢å¼• {index} è¶…å‡ºæ•°ç»„èŒƒå›´",)
            
            # æ£€æŸ¥ç›®æ ‡å…ƒç´ æ˜¯å¦ä¸ºå­—å…¸
            if not isinstance(target_element, dict):
                if return_empty_on_error:
                    return ("",)
                else:
                    return (f"é”™è¯¯: ç´¢å¼• {index} å¤„çš„å…ƒç´ ä¸æ˜¯å¯¹è±¡æ ¼å¼",)
            
            # æå–æŒ‡å®šå­—æ®µçš„å€¼
            field_value = target_element.get(field_name, "")
            
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²ç±»å‹
            if field_value is None:
                field_value = ""
            elif not isinstance(field_value, str):
                field_value = str(field_value)
            
            # æ˜¾ç¤ºæœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯
            actual_index = index if index >= 0 else len(data) + index
            print(f"JsonArrayElementFieldExtractor: ä»ç´¢å¼• {index} (å®é™…ä½ç½®: {actual_index}) æå–å­—æ®µ '{field_name}': {field_value}")
            
            return (field_value,)
            
        except Exception as e:
            error_msg = f"æå–å­—æ®µå€¼æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonArrayElementFieldExtractor error: {error_msg}")
            
            if return_empty_on_error:
                return ("",)
            else:
                return (error_msg,)


class DimensionReorderAndScale:
    """
    ä¸‰ç»´æ•°æ®é‡æ–°æ’åºå’Œç¼©æ”¾èŠ‚ç‚¹
    
    å¤„ç†æ ¼å¼å¦‚ [10, 10, 0.3] çš„ä¸‰ç»´æ•°æ®ï¼š
    â€¢ æ”¯æŒä»»æ„è°ƒæ¢é•¿(length)ã€å®½(width)ã€é«˜(height)çš„ä½ç½®
    â€¢ æä¾›ç¼©æ”¾å› å­æ§åˆ¶æ•´ä½“å¤§å°
    â€¢ è¾“å‡ºæ—¶é»˜è®¤ç§»é™¤æ–¹æ‹¬å·ï¼Œè¿”å›é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
    
    ğŸ“ è¾“å…¥æ ¼å¼æ”¯æŒï¼š
    â€¢ åˆ—è¡¨: [10, 10, 0.3]
    â€¢ JSONå­—ç¬¦ä¸²: "[10, 10, 0.3]"
    â€¢ é€—å·åˆ†éš”å­—ç¬¦ä¸²: "10, 10, 0.3"
    
    ğŸ”„ æ’åºé€‰é¡¹ï¼š
    â€¢ LWH (é•¿å®½é«˜) - é»˜è®¤é¡ºåº
    â€¢ LHW (é•¿é«˜å®½)
    â€¢ WLH (å®½é•¿é«˜)
    â€¢ WHL (å®½é«˜é•¿)
    â€¢ HLW (é«˜é•¿å®½)
    â€¢ HWL (é«˜å®½é•¿)
    
    âš™ï¸ ç¼©æ”¾æ§åˆ¶ï¼š
    â€¢ scale_factor: å…¨å±€ç¼©æ”¾å› å­ (é»˜è®¤ 1.0)
    â€¢ æ‰€æœ‰ä¸‰ä¸ªç»´åº¦éƒ½ä¼šä¹˜ä»¥æ­¤å› å­
    
    ğŸ“¤ è¾“å‡ºæ ¼å¼ï¼š
    â€¢ é»˜è®¤: "10.0,10.0,0.3" (é€—å·åˆ†éš”ï¼Œæ— ç©ºæ ¼)
    â€¢ å¯é€‰: "10.0, 10.0, 0.3" (ä¿ç•™ç©ºæ ¼)
    â€¢ å¯é€‰: "[10.0,10.0,0.3]" (ä¿ç•™æ–¹æ‹¬å·)
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "dimension_data": (IO.ANY, {"tooltip": "ä¸‰ç»´æ•°æ®è¾“å…¥\næ”¯æŒæ ¼å¼:\nâ€¢ [10, 10, 0.3]\nâ€¢ '[10, 10, 0.3]'\nâ€¢ '10, 10, 0.3'"}),
            },
            "optional": {
                "reorder_pattern": (["LWH", "LHW", "WLH", "WHL", "HLW", "HWL"], {
                    "default": "LWH",
                    "tooltip": "ç»´åº¦é‡æ’æ¨¡å¼\nâ€¢ L=é•¿åº¦(Length)\nâ€¢ W=å®½åº¦(Width) \nâ€¢ H=é«˜åº¦(Height)\n\nä¾‹å¦‚:\nâ€¢ LWH: é•¿-å®½-é«˜ (é»˜è®¤)\nâ€¢ WHL: å®½-é«˜-é•¿\nâ€¢ HLW: é«˜-é•¿-å®½"
                }),
                "scale_factor": (IO.FLOAT, {
                    "default": 1.0, 
                    "min": 0.001, 
                    "max": 1000.0, 
                    "step": 0.01,
                    "tooltip": "å…¨å±€ç¼©æ”¾å› å­\næ‰€æœ‰ç»´åº¦éƒ½ä¹˜ä»¥æ­¤å€¼\nâ€¢ 1.0 = ä¿æŒåŸå°ºå¯¸\nâ€¢ 0.5 = ç¼©å°ä¸€åŠ\nâ€¢ 2.0 = æ”¾å¤§ä¸€å€"
                }),
                "decimal_places": (IO.INT, {
                    "default": 2, 
                    "min": 0, 
                    "max": 6,
                    "tooltip": "å°æ•°ä½æ•°\næ§åˆ¶è¾“å‡ºæ•°å€¼çš„ç²¾åº¦\nâ€¢ 0: æ•´æ•° '10, 10, 0'\nâ€¢ 2: ä¸¤ä½å°æ•° '10.00, 10.00, 0.30'\nâ€¢ 3: ä¸‰ä½å°æ•° '10.000, 10.000, 0.300'"
                }),
                "keep_brackets": (IO.BOOLEAN, {
                    "default": False, 
                    "label_on": "ä¿ç•™[]", 
                    "label_off": "ç§»é™¤[]",
                    "tooltip": "è¾“å‡ºæ ¼å¼æ§åˆ¶\nâ€¢ å…³é—­: '10.0,10.0,0.3'\nâ€¢ å¼€å¯: '[10.0,10.0,0.3]'"
                }),
                "remove_spaces": (IO.BOOLEAN, {
                    "default": False, 
                    "label_on": "ç§»é™¤ç©ºæ ¼", 
                    "label_off": "ä¿ç•™ç©ºæ ¼",
                    "tooltip": "æ˜¯å¦ç§»é™¤è¾“å‡ºä¸­çš„æ‰€æœ‰ç©ºæ ¼\nâ€¢ å¼€å¯: '10.00,10.00,0.30'\nâ€¢ å…³é—­: '10.00, 10.00, 0.30'"
                }),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("reordered_dimensions",)
    FUNCTION = "reorder_and_scale"
    CATEGORY = "VVL/json"
    
    def _parse_dimension_data(self, dimension_data):
        """è§£æè¾“å…¥çš„ç»´åº¦æ•°æ®ä¸ºä¸‰ä¸ªæ•°å€¼çš„åˆ—è¡¨"""
        if dimension_data is None:
            return None
            
        # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(dimension_data, (list, tuple)) and len(dimension_data) >= 3:
            return [float(dimension_data[0]), float(dimension_data[1]), float(dimension_data[2])]
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        if isinstance(dimension_data, str):
            # ç§»é™¤å¯èƒ½çš„æ–¹æ‹¬å·
            clean_data = dimension_data.strip()
            if clean_data.startswith('[') and clean_data.endswith(']'):
                clean_data = clean_data[1:-1]
            
            # å°è¯•æŒ‰é€—å·åˆ†å‰²
            parts = clean_data.split(',')
            if len(parts) >= 3:
                try:
                    return [float(parts[0].strip()), float(parts[1].strip()), float(parts[2].strip())]
                except ValueError:
                    pass
            
            # å°è¯•ä½œä¸ºJSONè§£æ
            try:
                parsed = json.loads(dimension_data)
                if isinstance(parsed, (list, tuple)) and len(parsed) >= 3:
                    return [float(parsed[0]), float(parsed[1]), float(parsed[2])]
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _reorder_dimensions(self, lwh_values, pattern):
        """æ ¹æ®æŒ‡å®šæ¨¡å¼é‡æ–°æ’åˆ—ç»´åº¦"""
        length, width, height = lwh_values
        
        reorder_map = {
            "LWH": [length, width, height],   # é•¿å®½é«˜ (é»˜è®¤)
            "LHW": [length, height, width],   # é•¿é«˜å®½
            "WLH": [width, length, height],   # å®½é•¿é«˜
            "WHL": [width, height, length],   # å®½é«˜é•¿
            "HLW": [height, length, width],   # é«˜é•¿å®½
            "HWL": [height, width, length],   # é«˜å®½é•¿
        }
        
        return reorder_map.get(pattern, [length, width, height])
    
    def reorder_and_scale(self, dimension_data, reorder_pattern="LWH", scale_factor=1.0, keep_brackets=False, decimal_places=2, remove_spaces=True, **kwargs):
        """é‡æ–°æ’åºå’Œç¼©æ”¾ä¸‰ç»´æ•°æ®"""
        try:
            # è§£æè¾“å…¥æ•°æ®
            parsed_data = self._parse_dimension_data(dimension_data)
            
            if parsed_data is None:
                error_msg = "æ— æ³•è§£æè¾“å…¥çš„ä¸‰ç»´æ•°æ®ã€‚æœŸæœ›æ ¼å¼: [10, 10, 0.3] æˆ– '10, 10, 0.3'"
                print(f"DimensionReorderAndScale error: {error_msg}")
                return (error_msg,)
            
            # åº”ç”¨ç¼©æ”¾å› å­
            scaled_data = [value * scale_factor for value in parsed_data]
            
            # é‡æ–°æ’åˆ—ç»´åº¦
            reordered_data = self._reorder_dimensions(scaled_data, reorder_pattern)
            
            # æ ¼å¼åŒ–è¾“å‡º
            if decimal_places == 0:
                # æ•´æ•°æ ¼å¼
                formatted_values = [str(int(round(value))) for value in reordered_data]
            else:
                # å°æ•°æ ¼å¼
                format_str = f"{{:.{decimal_places}f}}"
                formatted_values = [format_str.format(value) for value in reordered_data]
            
            # ç»„è£…æœ€ç»ˆå­—ç¬¦ä¸²
            separator = "," if remove_spaces else ", "
            if keep_brackets:
                result = f"[{separator.join(formatted_values)}]"
            else:
                result = separator.join(formatted_values)
            
            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            print(f"DimensionReorderAndScale:")
            print(f"  è¾“å…¥: {dimension_data}")
            print(f"  è§£æå: {parsed_data}")
            print(f"  ç¼©æ”¾å› å­: {scale_factor}")
            print(f"  ç¼©æ”¾å: {scaled_data}")
            print(f"  é‡æ’æ¨¡å¼: {reorder_pattern}")
            print(f"  é‡æ’å: {reordered_data}")
            print(f"  æœ€ç»ˆè¾“å‡º: {result}")
            
            return (result,)
            
        except Exception as e:
            error_msg = f"å¤„ç†ä¸‰ç»´æ•°æ®æ—¶å‡ºé”™: {str(e)}"
            print(f"DimensionReorderAndScale error: {error_msg}")
            return (error_msg,)


# Node class mappings
NODE_CLASS_MAPPINGS = {
    "JsonObjectDeduplicator": JsonObjectDeduplicator,
    "JsonObjectMerger": JsonObjectMerger,
    "JsonExtractSubjectNamesScales": JsonExtractSubjectNamesScales,
    "ApplyUrlsToJson": ApplyUrlsToJson,
    "JsonMarkdownCleaner": JsonMarkdownCleaner,
    "IndexUrlPairDeduplicator": IndexUrlPairDeduplicator,
    "JsonArrayElementFieldExtractor": JsonArrayElementFieldExtractor,
    "DimensionReorderAndScale": DimensionReorderAndScale
}

# Node display name mappings
NODE_DISPLAY_NAME_MAPPINGS = {
    "JsonObjectDeduplicator": "VVL JSON Object Deduplicator",
    "JsonObjectMerger": "VVL JSON Object Merger",
    "JsonExtractSubjectNamesScales": "VVL JSON Extract: subject, names, scales",
    "ApplyUrlsToJson": "VVL Apply URLs to JSON",
    "JsonMarkdownCleaner": "VVL JSON Markdown Cleaner",
    "IndexUrlPairDeduplicator": "VVL Index-URL Pair Deduplicator",
    "JsonArrayElementFieldExtractor": "VVL JSON Array Element Field Extractor",
    "DimensionReorderAndScale": "VVL Dimension Reorder and Scale",
}