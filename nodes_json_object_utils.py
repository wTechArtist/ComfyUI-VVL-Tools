import json
import re
import ast
import copy
from comfy.comfy_types.node_typing import IO


class JsonObjectDeduplicator:
    """
    Remove duplicate objects from JSON based on name (excluding numeric suffix) and scale.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING, IO.STRING)
    RETURN_NAMES = ("deduplicated_json", "removed_duplicates")
    FUNCTION = "deduplicate_objects"
    CATEGORY = "VVL/json"
    
    def deduplicate_objects(self, json_text, **kwargs):
        try:
            # Parse input JSON
            data = json.loads(json_text)
            
            # Check if 'objects' field exists
            if 'objects' not in data:
                return json_text, json.dumps({"removed_objects": []})
            
            objects = data['objects']
            unique_objects = []
            removed_objects = []
            kept_original_indices = []
            
            # Track unique combinations of (base_name, scale)
            seen_combinations = {}
            
            for i, obj in enumerate(objects):
                # Extract base name (remove numeric suffix)
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                
                # Get scale as tuple for comparison
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                # Create combination key
                combination_key = (base_name, scale_tuple)
                
                if combination_key not in seen_combinations:
                    # First occurrence - keep it
                    seen_combinations[combination_key] = len(unique_objects)
                    unique_objects.append(obj)
                    kept_original_indices.append(i)
                else:
                    # Duplicate - add to removed list
                    removed_obj = obj.copy()
                    removed_obj['_original_index'] = i
                    removed_obj['_duplicate_of_index'] = seen_combinations[combination_key]
                    removed_objects.append(removed_obj)
            
            # Update data with deduplicated objects
            data['objects'] = unique_objects
            
            # Create result JSONs
            deduplicated_json = json.dumps(data, ensure_ascii=False, indent=2)
            removed_duplicates = json.dumps({
                "removed_objects": removed_objects,
                "original_count": len(objects),
                "deduplicated_count": len(unique_objects),
                "kept_original_indices": kept_original_indices
            }, ensure_ascii=False, indent=2)
            
            return deduplicated_json, removed_duplicates
            
        except json.JSONDecodeError as e:
            error_msg = f"Invalid JSON input: {str(e)}"
            return json.dumps({"error": error_msg}), json.dumps({"error": error_msg})
        except Exception as e:
            error_msg = f"Error processing JSON: {str(e)}"
            return json.dumps({"error": error_msg}), json.dumps({"error": error_msg})


class JsonObjectMerger:
    """
    Merge processed JSON with removed duplicates, copying processed fields (output.url, rotation, etc.) 
    to objects with same name and scale.
    
    Updates restored duplicate objects with fields from their corresponding processed objects:
    - output: Always copied if exists in processed object (including output.url)
    - 3d_url: For backward compatibility, also copied if exists
    - rotation: Always copied if exists in processed object
    - Additional fields can be easily added as needed
    
    This ensures all objects with the same base name and scale have consistent processed values.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "processed_json": (IO.STRING, {"multiline": True, "default": ""}),
                "removed_duplicates": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("merged_json",)
    FUNCTION = "merge_objects"
    CATEGORY = "VVL/json"
    
    def _copy_nested_field(self, source_obj, target_obj, field_path):
        """
        ä»æºå¯¹è±¡å¤åˆ¶åµŒå¥—å­—æ®µåˆ°ç›®æ ‡å¯¹è±¡
        æ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„ï¼Œå¦‚ "output.url"
        """
        if '.' not in field_path:
            # ç®€å•å­—æ®µï¼Œç›´æ¥å¤åˆ¶
            if field_path in source_obj:
                target_obj[field_path] = source_obj[field_path]
                return True
            return False
        
        # åˆ†å‰²è·¯å¾„
        path_parts = field_path.split('.')
        
        # æ£€æŸ¥æºå¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨è¯¥è·¯å¾„
        source_current = source_obj
        for part in path_parts[:-1]:
            if part not in source_current or not isinstance(source_current[part], dict):
                return False
            source_current = source_current[part]
        
        final_key = path_parts[-1]
        if final_key not in source_current:
            return False
        
        # åœ¨ç›®æ ‡å¯¹è±¡ä¸­åˆ›å»ºè·¯å¾„å¹¶å¤åˆ¶å€¼
        target_current = target_obj
        for part in path_parts[:-1]:
            if part not in target_current:
                target_current[part] = {}
            elif not isinstance(target_current[part], dict):
                # å¦‚æœä¸­é—´è·¯å¾„ä¸æ˜¯å­—å…¸ï¼Œæ— æ³•ç»§ç»­
                return False
            target_current = target_current[part]
        
        # å¤åˆ¶å€¼
        target_current[final_key] = source_current[final_key]
        return True
    
    def merge_objects(self, processed_json, removed_duplicates, **kwargs):
        try:
            # Parse input JSONs
            processed_data = json.loads(processed_json)
            removed_data = json.loads(removed_duplicates)
            
            # Check for errors in input
            if "error" in processed_data or "error" in removed_data:
                return (json.dumps({"error": "Input contains errors"}),)
            
            # Check if required fields exist
            if 'objects' not in processed_data or 'removed_objects' not in removed_data:
                return (processed_json,)
            
            processed_objects = processed_data['objects']
            removed_objects = removed_data['removed_objects']
            kept_original_indices = removed_data.get('kept_original_indices', [])
            original_count = removed_data.get('original_count', len(processed_objects) + len(removed_objects))
            
            # Create a mapping of (base_name, scale) to processed object data from processed objects
            object_data_mapping = {}
            for obj in processed_objects:
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                combination_key = (base_name, scale_tuple)
                # Store the entire processed object for reference
                object_data_mapping[combination_key] = obj
            
            # Restore removed objects with updated fields from corresponding processed objects
            restored_objects = []
            copy_count = 0
            
            for removed_obj in removed_objects:
                # Remove metadata fields
                obj = {k: v for k, v in removed_obj.items() 
                      if not k.startswith('_')}
                
                # Find matching processed object
                name = obj.get('name', '')
                base_name = re.sub(r'\d+$', '', name).strip()
                scale = obj.get('scale', [])
                scale_tuple = tuple(scale) if isinstance(scale, list) else scale
                
                combination_key = (base_name, scale_tuple)
                if combination_key in object_data_mapping:
                    processed_obj = object_data_mapping[combination_key]
                    
                    # Copy specific fields from processed object to restored object
                    # output - å¤åˆ¶æ•´ä¸ª output å¯¹è±¡ï¼ˆåŒ…æ‹¬ output.urlï¼‰
                    if 'output' in processed_obj:
                        obj['output'] = copy.deepcopy(processed_obj['output'])
                        copy_count += 1
                    
                    # 3d_url - ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿå¤åˆ¶æ­¤å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if '3d_url' in processed_obj:
                        obj['3d_url'] = processed_obj['3d_url']
                    
                    # rotation - copy if exists in processed object
                    # if 'rotation' in processed_obj:
                    #     obj['rotation'] = processed_obj['rotation']
                    
                    # You can add more fields here as needed
                    # Example: if 'position' in processed_obj:
                    #     obj['position'] = processed_obj['position']
                
                restored_objects.append(obj)
            
            # Reconstruct original order using kept_original_indices and _original_index
            full_objects = [None] * max(original_count, len(processed_objects) + len(restored_objects))

            # Place processed (kept) objects back to their original indices
            for idx, obj in enumerate(processed_objects):
                target_index = kept_original_indices[idx] if idx < len(kept_original_indices) else idx
                if 0 <= target_index < len(full_objects):
                    full_objects[target_index] = obj
                else:
                    # Fallback append if index out of bounds
                    full_objects.append(obj)

            # Place restored (previously removed) objects at their original indices
            for removed_obj, restored in zip(removed_objects, restored_objects):
                target_index = removed_obj.get('_original_index')
                if isinstance(target_index, int) and 0 <= target_index < len(full_objects):
                    full_objects[target_index] = restored
                else:
                    full_objects.append(restored)

            # Remove any None slots while preserving order
            all_objects = [obj for obj in full_objects if obj is not None]

            # Update data with all objects
            processed_data['objects'] = all_objects
            
            # Create final JSON
            merged_json = json.dumps(processed_data, ensure_ascii=False, indent=2)
            
            # è¾“å‡ºå¤„ç†ç»Ÿè®¡
            print(f"JsonObjectMerger å®Œæˆ:")
            print(f"  â€¢ æ¢å¤çš„å¯¹è±¡æ•°: {len(restored_objects)}")
            print(f"  â€¢ å¤åˆ¶ output å­—æ®µ: {copy_count} ä¸ª")
            print(f"  â€¢ æœ€ç»ˆå¯¹è±¡æ€»æ•°: {len(all_objects)}")
            
            return (merged_json,)
            
        except json.JSONDecodeError as e:
            error_msg = f"Invalid JSON input: {str(e)}"
            return (json.dumps({"error": error_msg}),)
        except Exception as e:
            error_msg = f"Error merging JSON: {str(e)}"
            return (json.dumps({"error": error_msg}),)


# Extract subject, names list, and scales list from JSON
class JsonExtractSubjectNamesScales:
    """
    Extracts subject, names list, scales list, and style from the input JSON.

    - names_json: JSON string array of names, optionally stripping numeric suffixes
    - scales_json: JSON string array of scale triplets (e.g., [[0.7,0.5,0.75], ...])
    - style: Style string extracted from scene.style field
    Returning JSON strings keeps strong compatibility with community nodes.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""}),
                "strip_numeric_suffix": (IO.BOOLEAN, {"default": True, "label_on": "strip", "label_off": "keep"}),
            }
        }

    RETURN_TYPES = (IO.STRING, IO.ANY, IO.ANY, IO.STRING)
    RETURN_NAMES = ("subject", "names_list", "scales_list", "style")
    FUNCTION = "extract_fields"
    CATEGORY = "VVL/json"

    def extract_fields(self, json_text, strip_numeric_suffix=True, **kwargs):
        try:
            data = json.loads(json_text)
            subject = data.get("subject", "")
            objects = data.get("objects", [])
            
            # Extract style from scene field
            scene = data.get("scene", {})
            style = scene.get("style", "") if isinstance(scene, dict) else ""

            names = []
            scales = []
            for obj in objects:
                name = obj.get("name", "")
                if strip_numeric_suffix:
                    name = re.sub(r"\d+$", "", name).strip()
                names.append(name)
                scales.append(obj.get("scale", []))

            return (
                subject,
                names,
                scales,
                style,
            )
        except json.JSONDecodeError as e:
            msg = json.dumps({"error": f"Invalid JSON input: {str(e)}"})
            return (msg, msg, msg, msg)
        except Exception as e:
            msg = json.dumps({"error": f"Error extracting fields: {str(e)}"})
            return (msg, msg, msg, msg)


class ApplyUrlsToJson:
    """
    Apply a list of (index, url) pairs to JSON objects with nested field path support.

    - Supports nested field paths like "output.url" or simple fields like "3d_url"
    - Automatically creates intermediate objects if they don't exist
    - Robust to inputs shaped like [[idx, url], ...]
    - Skips entries with None url (configurable)
    - Ignores out-of-bounds indices (configurable)
    
    Example nested field paths:
    - "output.url" â†’ objects[idx].output.url = value
    - "3d_url" â†’ objects[idx].3d_url = value (backward compatible)
    - "data.result.file_path" â†’ objects[idx].data.result.file_path = value
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "deduplicated_json": (IO.STRING, {"multiline": True, "default": ""}),
                "idx_url_list": (IO.ANY, {}),
            },
            "optional": {
                "field_name": (IO.STRING, {"default": "output.url", "tooltip": "å­—æ®µè·¯å¾„\nâ€¢ æ”¯æŒåµŒå¥—è·¯å¾„: 'output.url'\nâ€¢ æ”¯æŒç®€å•å­—æ®µ: '3d_url'\nâ€¢ è‡ªåŠ¨åˆ›å»ºä¸­é—´å¯¹è±¡"}),
                "skip_none_url": (IO.BOOLEAN, {"default": True, "label_on": "skip", "label_off": "write None"}),
                "ignore_oob_index": (IO.BOOLEAN, {"default": True, "label_on": "ignore", "label_off": "error"}),
            },
        }

    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("processed_json",)
    FUNCTION = "apply"
    CATEGORY = "VVL/json"

    def _normalize_pairs(self, idx_url_list):
        # Debug: print what we actually received
        print(f"ApplyUrlsToJson received: {type(idx_url_list)} = {idx_url_list}")
        
        pairs = []
        if idx_url_list is None:
            return pairs

        # Handle various input formats from different ComfyUI node outputs
        container = idx_url_list
        if not isinstance(container, (list, tuple)):
            container = [container]

        # Special case: check if container itself is a single [idx, url] pair
        if (isinstance(container, (list, tuple)) and len(container) == 2 and
            not isinstance(container[0], (list, tuple, str)) and  # first element is not nested
            isinstance(container[1], str)):  # second element is string (url)
            try:
                idx = int(container[0])
                url = container[1]
                pairs.append((idx, url))
                print(f"ApplyUrlsToJson detected single pair: [{idx}, '{url}']")
                print(f"ApplyUrlsToJson normalized to: {pairs}")
                return pairs
            except (ValueError, TypeError):
                # If conversion fails, fall through to normal processing
                pass

        for item in container:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                # Direct [idx, url] pair
                pairs.append((item[0], item[1]))
            elif isinstance(item, str) and "," in item:
                # CR_IntertwineLists style: "idx, url"
                parts = item.split(",", 1)
                if len(parts) == 2:
                    try:
                        idx = int(parts[0].strip())
                        url = parts[1].strip()
                        pairs.append((idx, url))
                    except ValueError:
                        continue
            elif hasattr(item, '__iter__') and not isinstance(item, str):
                # Nested containers - recursive flatten
                for subitem in item:
                    if isinstance(subitem, (list, tuple)) and len(subitem) >= 2:
                        pairs.append((subitem[0], subitem[1]))
        
        print(f"ApplyUrlsToJson normalized to: {pairs}")
        return pairs
    
    def _set_nested_field(self, obj, field_path, value):
        """
        è®¾ç½®åµŒå¥—å­—æ®µå€¼ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„
        ä¾‹å¦‚: "output.url" ä¼šè®¾ç½® obj["output"]["url"] = value
        å¦‚æœä¸­é—´å¯¹è±¡ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»º
        """
        if not isinstance(obj, dict):
            return False
        
        # å¦‚æœæ²¡æœ‰ç‚¹å·ï¼Œç›´æ¥è®¾ç½®
        if '.' not in field_path:
            obj[field_path] = value
            return True
        
        # åˆ†å‰²è·¯å¾„
        path_parts = field_path.split('.')
        current = obj
        
        # éå†åˆ°å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼Œåˆ›å»º/è®¿é—®ä¸­é—´å¯¹è±¡
        for part in path_parts[:-1]:
            if part not in current:
                # åˆ›å»ºä¸­é—´å¯¹è±¡
                current[part] = {}
            elif not isinstance(current[part], dict):
                # ä¸­é—´è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯å­—å…¸ï¼Œæ— æ³•ç»§ç»­
                print(f"ApplyUrlsToJson: å­—æ®µè·¯å¾„ '{field_path}' ä¸­çš„ '{part}' ä¸æ˜¯å¯¹è±¡ç±»å‹ï¼Œæ— æ³•è®¾ç½®åµŒå¥—å€¼")
                return False
            current = current[part]
        
        # è®¾ç½®æœ€åçš„å­—æ®µ
        final_key = path_parts[-1]
        current[final_key] = value
        return True

    def apply(self, deduplicated_json, idx_url_list, field_name="output.url", skip_none_url=True, ignore_oob_index=True, **kwargs):
        try:
            data = json.loads(deduplicated_json)
        except json.JSONDecodeError as e:
            return (json.dumps({"error": f"Invalid JSON input: {str(e)}"}),)

        if not isinstance(data, dict) or "objects" not in data or not isinstance(data["objects"], list):
            return (deduplicated_json,)

        objects = data["objects"]

        pairs = self._normalize_pairs(idx_url_list)
        success_count = 0
        failed_count = 0
        
        for raw_idx, url in pairs:
            try:
                idx = int(raw_idx)
            except Exception:
                failed_count += 1
                continue

            if idx < 0 or idx >= len(objects):
                if ignore_oob_index:
                    failed_count += 1
                    continue
                else:
                    return (json.dumps({"error": f"Index out of bounds: {idx}"}, ensure_ascii=False),)

            if url is None and skip_none_url:
                continue

            obj = objects[idx]
            if isinstance(obj, dict):
                if self._set_nested_field(obj, field_name, url):
                    success_count += 1
                else:
                    failed_count += 1
        
        # è¾“å‡ºå¤„ç†ç»Ÿè®¡
        print(f"ApplyUrlsToJson å®Œæˆ:")
        print(f"  â€¢ å­—æ®µè·¯å¾„: '{field_name}'")
        print(f"  â€¢ æˆåŠŸè®¾ç½®: {success_count} ä¸ª")
        if failed_count > 0:
            print(f"  â€¢ å¤±è´¥/è·³è¿‡: {failed_count} ä¸ª")

        processed_json = json.dumps(data, ensure_ascii=False, indent=2)
        return (processed_json,)




class JsonMarkdownCleaner:
    """
    æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„ Markdown ä»£ç å—æ ‡è®°ã€‚
    
    ç§»é™¤ï¼š
    - å¼€å¤´çš„ ```json æˆ– ```
    - ç»“å°¾çš„ ```
    - ä¿æŒ JSON å†…å®¹å®Œæ•´
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": ""})
            }
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("cleaned_json",)
    FUNCTION = "clean_json"
    CATEGORY = "VVL/json"
    
    def clean_json(self, json_text, **kwargs):
        """æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„ Markdown ä»£ç å—æ ‡è®°"""
        try:
            cleaned_text = json_text.strip()
            
            # ç§»é™¤å¼€å¤´çš„ ```json æˆ– ```
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:].strip()
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:].strip()
            
            # ç§»é™¤ç»“å°¾çš„ ```
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3].strip()
            
            # éªŒè¯æ¸…ç†åçš„æ–‡æœ¬æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
            try:
                json.loads(cleaned_text)
            except json.JSONDecodeError as e:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œè¿”å›è­¦å‘Šä½†ä¸é˜»æ­¢å¤„ç†
                print(f"Warning: Cleaned text may not be valid JSON: {str(e)}")
            
            return (cleaned_text,)
            
        except Exception as e:
            error_msg = f"æ¸…ç† JSON æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonMarkdownCleaner error: {error_msg}")
            return (json_text,)  # å‡ºé”™æ—¶è¿”å›åŸå§‹æ–‡æœ¬


class IndexUrlPairDeduplicator:
    """
    ç§»é™¤ (index, url) å¯¹åˆ—è¡¨ä¸­çš„é‡å¤é¡¹ã€‚
    
    æ”¯æŒä¸‰ç§ç­–ç•¥ï¼š
    - remove_all: å®Œå…¨åˆ é™¤æ‰€æœ‰é‡å¤é¡¹ï¼ˆåŒ…æ‹¬ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
    - keep_first: ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„é‡å¤é¡¹
    - keep_last: ä¿ç•™æœ€åä¸€ä¸ªå‡ºç°çš„é‡å¤é¡¹
    
    æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œå¦‚ [[idx, url], ...] æˆ– ["idx,url", ...]
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "idx_url_list": (IO.ANY, {}),
            },
            "optional": {
                "preserve_order": (IO.BOOLEAN, {"default": True, "label_on": "ä¿æŒé¡ºåº", "label_off": "ä¸ä¿è¯é¡ºåº"}),
                "duplicate_strategy": (["keep_first", "keep_last", "remove_all"], {"default": "remove_all"}),
            },
        }
    
    RETURN_TYPES = (IO.ANY,)
    RETURN_NAMES = ("deduplicated_pairs",)
    FUNCTION = "deduplicate_pairs"
    CATEGORY = "VVL/json"
    
    def _normalize_pairs(self, idx_url_list):
        """è§„èŒƒåŒ–è¾“å…¥æ•°æ®ä¸º (index, url) å¯¹çš„åˆ—è¡¨"""
        pairs = []
        if idx_url_list is None:
            return pairs

        container = idx_url_list
        if not isinstance(container, (list, tuple)):
            container = [container]

        for item in container:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                # ç›´æ¥çš„ [idx, url] å¯¹
                pairs.append((item[0], item[1]))
            elif isinstance(item, str) and "," in item:
                # CR_IntertwineLists é£æ ¼: "idx, url"
                parts = item.split(",", 1)
                if len(parts) == 2:
                    try:
                        idx = int(parts[0].strip())
                        url = parts[1].strip()
                        pairs.append((idx, url))
                    except ValueError:
                        continue
            elif hasattr(item, '__iter__') and not isinstance(item, str):
                # åµŒå¥—å®¹å™¨ - é€’å½’å±•å¼€
                for subitem in item:
                    if isinstance(subitem, (list, tuple)) and len(subitem) >= 2:
                        pairs.append((subitem[0], subitem[1]))
        
        return pairs
    
    def deduplicate_pairs(self, idx_url_list, preserve_order=True, duplicate_strategy="remove_all", **kwargs):
        """å»é™¤é‡å¤çš„ (index, url) å¯¹"""
        try:
            pairs = self._normalize_pairs(idx_url_list)
            
            if not pairs:
                return (idx_url_list,)
            
            # ç»Ÿè®¡æ¯ä¸ªé”®å‡ºç°çš„æ¬¡æ•°
            key_counts = {}
            for idx, url in pairs:
                key = (idx, url)
                key_counts[key] = key_counts.get(key, 0) + 1
            
            if duplicate_strategy == "remove_all":
                # ç§»é™¤æ‰€æœ‰é‡å¤é¡¹ï¼ˆåŒ…æ‹¬ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
                if preserve_order:
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        # åªä¿ç•™å‡ºç°æ¬¡æ•°ä¸º1çš„é¡¹
                        if key_counts[key] == 1:
                            deduplicated.append([idx, url])
                else:
                    # ä¸ä¿è¯é¡ºåºï¼Œç›´æ¥è¿‡æ»¤
                    unique_keys = {k: [k[0], k[1]] for k, count in key_counts.items() if count == 1}
                    deduplicated = list(unique_keys.values())
                    
            elif duplicate_strategy == "keep_first":
                # ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„
                if preserve_order:
                    seen = set()
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        if key not in seen:
                            seen.add(key)
                            deduplicated.append([idx, url])
                else:
                    seen = {}
                    for idx, url in pairs:
                        key = (idx, url)
                        if key not in seen:
                            seen[key] = [idx, url]
                    deduplicated = list(seen.values())
                    
            elif duplicate_strategy == "keep_last":
                # ä¿ç•™æœ€åä¸€ä¸ªå‡ºç°çš„
                if preserve_order:
                    seen = set()
                    deduplicated = []
                    for idx, url in pairs:
                        key = (idx, url)
                        if key in seen:
                            # ç§»é™¤ä¹‹å‰æ·»åŠ çš„ç›¸åŒé¡¹
                            deduplicated = [pair for pair in deduplicated 
                                          if (pair[0], pair[1]) != key]
                        seen.add(key)
                        deduplicated.append([idx, url])
                else:
                    # ä¸ä¿è¯é¡ºåºï¼Œä¿ç•™æœ€åä¸€ä¸ª
                    seen = {}
                    for idx, url in pairs:
                        key = (idx, url)
                        seen[key] = [idx, url]
                    deduplicated = list(seen.values())
            
            print(f"IndexUrlPairDeduplicator: è¾“å…¥ {len(pairs)} å¯¹ï¼Œè¾“å‡º {len(deduplicated)} å¯¹")
            print(f"ç­–ç•¥: {duplicate_strategy}")
            print(f"å»é‡å‰: {pairs}")
            print(f"å»é‡å: {deduplicated}")
            
            # è¾“å‡ºé‡å¤é¡¹ç»Ÿè®¡ä¿¡æ¯
            duplicates = {k: count for k, count in key_counts.items() if count > 1}
            if duplicates:
                print(f"å‘ç°çš„é‡å¤é¡¹: {duplicates}")
            
            return (deduplicated,)
            
        except Exception as e:
            error_msg = f"å»é‡å¤„ç†é”™è¯¯: {str(e)}"
            print(f"IndexUrlPairDeduplicator error: {error_msg}")
            return (idx_url_list,)


class JsonArrayElementFieldExtractor:
    """
    ä»JSONæ•°ç»„ä¸­æå–æŒ‡å®šç´¢å¼•å…ƒç´ çš„æŒ‡å®šå­—æ®µå€¼ã€‚
    
    ğŸ“‹ æ”¯æŒè¾“å…¥æ ¼å¼ï¼š
    â€¢ JSONå­—ç¬¦ä¸²æ•°ç»„: '[{"name":"file1","url":"path1"}, ...]'
    â€¢ Pythonåˆ—è¡¨å¯¹è±¡: [{"name":"file1","url":"path1"}, ...]
    
    ğŸ¯ ç´¢å¼•ç”¨æ³•è¯¦è§£ï¼š
    â€¢ æ­£æ•°ç´¢å¼•: ä»æ•°ç»„å¼€å¤´è®¡æ•°
      - 0 = ç¬¬1ä¸ªå…ƒç´ 
      - 1 = ç¬¬2ä¸ªå…ƒç´   
      - 2 = ç¬¬3ä¸ªå…ƒç´  ...
    
    â€¢ è´Ÿæ•°ç´¢å¼•: ä»æ•°ç»„æœ«å°¾è®¡æ•°
      - -1 = æœ€å1ä¸ªå…ƒç´ 
      - -2 = å€’æ•°ç¬¬2ä¸ªå…ƒç´ 
      - -3 = å€’æ•°ç¬¬3ä¸ªå…ƒç´  ...
    
    ğŸ’¡ å®ç”¨ç¤ºä¾‹ï¼š
    æ•°ç»„: ["A", "B", "C", "D", "E"]
    ç´¢å¼•å¯¹ç…§è¡¨:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æ­£ç´¢å¼•  â”‚    0    â”‚    1    â”‚    2    â”‚    3    â”‚    4    â”‚
    â”‚ å…ƒç´ å€¼  â”‚   "A"   â”‚   "B"   â”‚   "C"   â”‚   "D"   â”‚   "E"   â”‚
    â”‚ è´Ÿç´¢å¼•  â”‚   -5    â”‚   -4    â”‚   -3    â”‚   -2    â”‚   -1    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    âš™ï¸ å¯é…ç½®å‚æ•°ï¼š
    â€¢ index: æ•°ç»„ç´¢å¼•ä½ç½®ï¼ˆæ”¯æŒæ­£è´Ÿæ•°ï¼‰
    â€¢ field_name: è¦æå–çš„å­—æ®µåç§°
    
    âš ï¸ é”™è¯¯å¤„ç†ï¼š
    â€¢ å‡ºé”™æ—¶ç»Ÿä¸€è¿”å›å­—ç¬¦ä¸² "None"
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_array": (IO.ANY, {}),
            },
            "optional": {
                "index": (IO.INT, {"default": 0, "min": -999999, "max": 999999, "tooltip": "æ•°ç»„ç´¢å¼•ä½ç½®\nâ€¢ æ­£æ•°: ä»å¤´å¼€å§‹ (0=ç¬¬1ä¸ª, 1=ç¬¬2ä¸ª, ...)\nâ€¢ è´Ÿæ•°: ä»å°¾å¼€å§‹ (-1=æœ€å1ä¸ª, -2=å€’æ•°ç¬¬2ä¸ª, ...)\nâ€¢ ç¤ºä¾‹: æœ‰5ä¸ªå…ƒç´ çš„æ•°ç»„\n  ç´¢å¼• 0,1,2,3,4 å¯¹åº” ç¬¬1,2,3,4,5ä¸ªå…ƒç´ \n  ç´¢å¼• -1,-2,-3,-4,-5 å¯¹åº” æœ€å1,2,3,4,5ä¸ªå…ƒç´ "}),
                "field_name": (IO.STRING, {"default": "url", "tooltip": "è¦æå–çš„å­—æ®µåç§°\nâ€¢ é»˜è®¤: 'url'\nâ€¢ å¸¸ç”¨å­—æ®µ: name, cover, id, path ç­‰\nâ€¢ æ”¯æŒä»»æ„å¯¹è±¡å±æ€§å"}),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("field_value",)
    FUNCTION = "extract_field_value"
    CATEGORY = "VVL/json"
    
    def extract_field_value(self, json_array, index=0, field_name="url", **kwargs):
        """ä»æ•°ç»„ä¸­æå–æŒ‡å®šç´¢å¼•å…ƒç´ çš„å­—æ®µå€¼"""
        try:
            # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸² "None"ï¼Œç›´æ¥è¿”å› "None"
            if json_array == "None":
                return ("None",)
            
            # å¤„ç†è¾“å…¥æ•°æ®
            data = json_array
            
            # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            if isinstance(json_array, str):
                try:
                    data = json.loads(json_array)
                except json.JSONDecodeError:
                    return ("None",)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨/æ•°ç»„
            if not isinstance(data, list):
                return ("None",)
            
            # æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º
            if len(data) == 0:
                return ("None",)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            try:
                # Pythonæ”¯æŒè´Ÿæ•°ç´¢å¼•ï¼Œä½†æˆ‘ä»¬éœ€è¦æ£€æŸ¥è¾¹ç•Œ
                if index >= len(data) or index < -len(data):
                    return ("None",)
                
                # è·å–æŒ‡å®šç´¢å¼•çš„å…ƒç´ 
                target_element = data[index]
                
            except IndexError:
                return ("None",)
            
            # æ£€æŸ¥ç›®æ ‡å…ƒç´ æ˜¯å¦ä¸ºå­—å…¸
            if not isinstance(target_element, dict):
                return ("None",)
            
            # æå–æŒ‡å®šå­—æ®µçš„å€¼
            field_value = target_element.get(field_name, "")
            
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²ç±»å‹
            if field_value is None:
                field_value = ""
            elif not isinstance(field_value, str):
                field_value = str(field_value)
            
            # æ˜¾ç¤ºæœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯
            actual_index = index if index >= 0 else len(data) + index
            print(f"JsonArrayElementFieldExtractor: ä»ç´¢å¼• {index} (å®é™…ä½ç½®: {actual_index}) æå–å­—æ®µ '{field_name}': {field_value}")
            
            return (field_value,)
            
        except Exception as e:
            error_msg = f"æå–å­—æ®µå€¼æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonArrayElementFieldExtractor error: {error_msg}")
            return ("None",)


class JsonRotationScaleAdjuster:
    """
    JSONæ—‹è½¬å€¼å’Œç¼©æ”¾è°ƒæ•´å™¨
    
    å¯¹JSONæ•°æ®ä¸­æ‰€æœ‰å¯¹è±¡çš„rotationå’Œscaleå€¼è¿›è¡Œæ‰¹é‡è°ƒæ•´ï¼š
    â€¢ æ”¯æŒå¯¹Xã€Yã€Zè½´æ—‹è½¬å€¼åˆ†åˆ«è¿›è¡ŒåŠ å‡æ“ä½œ
    â€¢ æ”¯æŒå¯¹Xã€Yã€Zè½´ç¼©æ”¾å€¼åˆ†åˆ«è¿›è¡Œä¹˜æ³•æ“ä½œ
    â€¢ å¯å¤„ç†åœºæ™¯JSONæ–‡ä»¶ä¸­çš„æ‰€æœ‰objects
    â€¢ ä¿æŒåŸæœ‰çš„æ•°æ®ç»“æ„å’Œæ ¼å¼
    
    ğŸ”„ åŠŸèƒ½ç‰¹æ€§ï¼š
    â€¢ æ‰¹é‡å¤„ç†ï¼šä¸€æ¬¡æ€§è°ƒæ•´æ‰€æœ‰å¯¹è±¡çš„æ—‹è½¬å€¼å’Œç¼©æ”¾å€¼
    â€¢ è½´å‘æ§åˆ¶ï¼šåˆ†åˆ«æ§åˆ¶Xã€Yã€Zè½´çš„æ—‹è½¬åç§»å’Œç¼©æ”¾å› å­
    â€¢ æ—‹è½¬èŒƒå›´ï¼šæ¯ä¸ªè½´çš„è°ƒæ•´èŒƒå›´ä¸º-360Â°åˆ°+360Â°
    â€¢ ç¼©æ”¾èŒƒå›´ï¼šæ¯ä¸ªè½´çš„ç¼©æ”¾å› å­èŒƒå›´ä¸º0.001åˆ°1000
    â€¢ å®‰å…¨å¤„ç†ï¼šè‡ªåŠ¨è·³è¿‡æ²¡æœ‰ç›¸åº”å­—æ®µçš„å¯¹è±¡
    
    âš™ï¸ å‚æ•°è¯´æ˜ï¼š
    â€¢ rotation_order: æ—‹è½¬å€¼é¡ºåºé‡æ’ï¼ˆXYZ, XZY, YXZ, YZX, ZXY, ZYXï¼‰
    â€¢ rotation_x_offset: Xè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰
    â€¢ rotation_y_offset: Yè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰  
    â€¢ rotation_z_offset: Zè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰
    â€¢ scale_x_multiplier: Xè½´ç¼©æ”¾ä¹˜æ³•å› å­
    â€¢ scale_y_multiplier: Yè½´ç¼©æ”¾ä¹˜æ³•å› å­
    â€¢ scale_z_multiplier: Zè½´ç¼©æ”¾ä¹˜æ³•å› å­
    
    ğŸ“ ä½¿ç”¨åœºæ™¯ï¼š
    â€¢ åœºæ™¯æ•´ä½“æ—‹è½¬è°ƒæ•´
    â€¢ è§†è§’æ–¹å‘ä¿®æ­£
    â€¢ æ‰¹é‡å¯¹è±¡æœå‘è°ƒæ•´
    â€¢ åæ ‡ç³»è½¬æ¢è¡¥å¿
    â€¢ æ‰¹é‡å°ºå¯¸ç¼©æ”¾è°ƒæ•´
    â€¢ æ¯”ä¾‹ä¿®æ­£å’Œé€‚é…
    
    ğŸ’¡ æ³¨æ„äº‹é¡¹ï¼š
    â€¢ åªå¤„ç†åŒ…å«ç›¸åº”å­—æ®µä¸”æ ¼å¼æ­£ç¡®çš„å¯¹è±¡
    â€¢ rotationå’Œscaleå­—æ®µå¿…é¡»æ˜¯é•¿åº¦â‰¥3çš„æ•°ç»„
    â€¢ rotationé¡ºåºè°ƒæ¢åœ¨åç§»é‡è®¡ç®—ä¹‹å‰æ‰§è¡Œ
    â€¢ è§’åº¦è®¡ç®—ä¸ä¼šè‡ªåŠ¨è§„èŒƒåŒ–åˆ°0-360èŒƒå›´
    â€¢ ç¼©æ”¾ä½¿ç”¨ä¹˜æ³•ï¼Œ1.0è¡¨ç¤ºä¿æŒåŸå°ºå¯¸
    
    ğŸ”„ æ—‹è½¬é¡ºåºè¯´æ˜ï¼š
    â€¢ XYZ: [x, y, z] - é»˜è®¤é¡ºåº
    â€¢ XZY: [x, z, y] - Xä¸å˜ï¼ŒYå’ŒZäº’æ¢
    â€¢ YXZ: [y, x, z] - Xå’ŒYäº’æ¢ï¼ŒZä¸å˜
    â€¢ YZX: [y, z, x] - Yâ†’X, Zâ†’Y, Xâ†’Z
    â€¢ ZXY: [z, x, y] - Zâ†’X, Xâ†’Y, Yâ†’Z
    â€¢ ZYX: [z, y, x] - Zå’ŒXäº’æ¢ï¼ŒYä¸å˜
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": "", "tooltip": "åŒ…å«objectsæ•°ç»„çš„JSONæ•°æ®\næ”¯æŒåœºæ™¯æè¿°æ–‡ä»¶æ ¼å¼\nå°†å¯¹æ‰€æœ‰å¯¹è±¡çš„rotationå’Œscaleå€¼è¿›è¡Œè°ƒæ•´"})
            },
            "optional": {
                "rotation_order": (["XYZ", "XZY", "YXZ", "YZX", "ZXY", "ZYX"], {
                    "default": "YZX",
                    "tooltip": "æ—‹è½¬å€¼çš„é¡ºåºé‡æ’\nâ€¢ XYZ: [x, y, z] - é»˜è®¤é¡ºåº\nâ€¢ XZY: [x, z, y] - Yå’ŒZäº’æ¢\nâ€¢ YXZ: [y, x, z] - Xå’ŒYäº’æ¢\nâ€¢ YZX: [y, z, x] - å¾ªç¯å³ç§»\nâ€¢ ZXY: [z, x, y] - å¾ªç¯å·¦ç§»\nâ€¢ ZYX: [z, y, x] - Xå’ŒZäº’æ¢\næ³¨æ„ï¼šé‡æ’åœ¨åç§»é‡è®¡ç®—ä¹‹å‰æ‰§è¡Œ"
                }),
                "rotation_x_offset": (IO.FLOAT, {
                    "default": 0.0, 
                    "min": -360.0, 
                    "max": 360.0, 
                    "step": 0.1,
                    "tooltip": "Xè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰\næ­£å€¼ï¼šç»•Xè½´æ­£æ–¹å‘æ—‹è½¬\nè´Ÿå€¼ï¼šç»•Xè½´è´Ÿæ–¹å‘æ—‹è½¬\nèŒƒå›´ï¼š-360Â° åˆ° +360Â°\næ³¨æ„ï¼šåœ¨rotation_orderé‡æ’ååº”ç”¨"
                }),
                "rotation_y_offset": (IO.FLOAT, {
                    "default": -90.0, 
                    "min": -360.0, 
                    "max": 360.0, 
                    "step": 0.1,
                    "tooltip": "Yè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰\næ­£å€¼ï¼šç»•Yè½´æ­£æ–¹å‘æ—‹è½¬\nè´Ÿå€¼ï¼šç»•Yè½´è´Ÿæ–¹å‘æ—‹è½¬\nèŒƒå›´ï¼š-360Â° åˆ° +360Â°\næ³¨æ„ï¼šåœ¨rotation_orderé‡æ’ååº”ç”¨"
                }),
                "rotation_z_offset": (IO.FLOAT, {
                    "default": 0.0, 
                    "min": -360.0, 
                    "max": 360.0, 
                    "step": 0.1,
                    "tooltip": "Zè½´æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰\næ­£å€¼ï¼šç»•Zè½´æ­£æ–¹å‘æ—‹è½¬\nè´Ÿå€¼ï¼šç»•Zè½´è´Ÿæ–¹å‘æ—‹è½¬\nèŒƒå›´ï¼š-360Â° åˆ° +360Â°\næ³¨æ„ï¼šåœ¨rotation_orderé‡æ’ååº”ç”¨"
                }),
                "scale_x_multiplier": (IO.FLOAT, {
                    "default": 0.01, 
                    "min": 0.001, 
                    "max": 1000.0, 
                    "step": 0.001,
                    "tooltip": "Xè½´ç¼©æ”¾ä¹˜æ³•å› å­\n1.0ï¼šä¿æŒåŸå°ºå¯¸\n0.5ï¼šç¼©å°ä¸€åŠ\n2.0ï¼šæ”¾å¤§ä¸€å€\nèŒƒå›´ï¼š0.001 åˆ° 1000"
                }),
                "scale_y_multiplier": (IO.FLOAT, {
                    "default": 0.01, 
                    "min": 0.001, 
                    "max": 1000.0, 
                    "step": 0.001,
                    "tooltip": "Yè½´ç¼©æ”¾ä¹˜æ³•å› å­\n1.0ï¼šä¿æŒåŸå°ºå¯¸\n0.5ï¼šç¼©å°ä¸€åŠ\n2.0ï¼šæ”¾å¤§ä¸€å€\nèŒƒå›´ï¼š0.001 åˆ° 1000"
                }),
                "scale_z_multiplier": (IO.FLOAT, {
                    "default": 0.01, 
                    "min": 0.001, 
                    "max": 1000.0, 
                    "step": 0.001,
                    "tooltip": "Zè½´ç¼©æ”¾ä¹˜æ³•å› å­\n1.0ï¼šä¿æŒåŸå°ºå¯¸\n0.5ï¼šç¼©å°ä¸€åŠ\n2.0ï¼šæ”¾å¤§ä¸€å€\nèŒƒå›´ï¼š0.001 åˆ° 1000"
                }),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("adjusted_json",)
    FUNCTION = "adjust_rotations_and_scales"
    CATEGORY = "VVL/json"
    
    def _reorder_rotation(self, rotation_values, order):
        """æ ¹æ®æŒ‡å®šé¡ºåºé‡æ–°æ’åˆ—rotationå€¼"""
        x, y, z = rotation_values[0], rotation_values[1], rotation_values[2]
        
        order_map = {
            "XYZ": [x, y, z],  # é»˜è®¤é¡ºåº
            "XZY": [x, z, y],  # Yå’ŒZäº’æ¢
            "YXZ": [y, x, z],  # Xå’ŒYäº’æ¢
            "YZX": [y, z, x],  # å¾ªç¯å³ç§»ï¼šYâ†’X, Zâ†’Y, Xâ†’Z
            "ZXY": [z, x, y],  # å¾ªç¯å·¦ç§»ï¼šZâ†’X, Xâ†’Y, Yâ†’Z
            "ZYX": [z, y, x],  # Xå’ŒZäº’æ¢
        }
        
        return order_map.get(order, [x, y, z])

    def adjust_rotations_and_scales(self, json_text, rotation_order="XYZ", rotation_x_offset=0.0, rotation_y_offset=0.0, rotation_z_offset=0.0, 
                                   scale_x_multiplier=1.0, scale_y_multiplier=1.0, scale_z_multiplier=1.0, **kwargs):
        """è°ƒæ•´JSONä¸­æ‰€æœ‰å¯¹è±¡çš„rotationå’Œscaleå€¼"""
        try:
            # è§£æè¾“å…¥JSON
            data = json.loads(json_text)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰objectså­—æ®µ
            if 'objects' not in data or not isinstance(data['objects'], list):
                print("JsonRotationScaleAdjuster: æœªæ‰¾åˆ°æœ‰æ•ˆçš„objectsæ•°ç»„")
                return (json_text,)
            
            objects = data['objects']
            rotation_processed = 0
            scale_processed = 0
            skipped_count = 0
            
            # éå†æ‰€æœ‰å¯¹è±¡å¹¶è°ƒæ•´rotationå’Œscaleå€¼
            for i, obj in enumerate(objects):
                if not isinstance(obj, dict):
                    skipped_count += 1
                    continue
                
                obj_processed = False
                
                # å¤„ç†rotationå­—æ®µ
                if 'rotation' in obj:
                    rotation = obj['rotation']
                    
                    # æ£€æŸ¥rotationæ˜¯å¦ä¸ºæœ‰æ•ˆçš„åˆ—è¡¨æ ¼å¼
                    if isinstance(rotation, list) and len(rotation) >= 3:
                        try:
                            # ç¡®ä¿åŸå§‹å€¼æ˜¯æ•°å€¼ç±»å‹
                            original_values = [float(rotation[0]), float(rotation[1]), float(rotation[2])]
                            
                            # 1. é¦–å…ˆæŒ‰ç…§æŒ‡å®šé¡ºåºé‡æ–°æ’åˆ—rotationå€¼
                            reordered_values = self._reorder_rotation(original_values, rotation_order)
                            
                            # 2. ç„¶ååœ¨é‡æ’åçš„å€¼ä¸Šåº”ç”¨åç§»é‡
                            new_x = reordered_values[0] + rotation_x_offset
                            new_y = reordered_values[1] + rotation_y_offset
                            new_z = reordered_values[2] + rotation_z_offset
                            
                            # æ›´æ–°rotationå€¼
                            obj['rotation'][0] = new_x
                            obj['rotation'][1] = new_y
                            obj['rotation'][2] = new_z
                            
                            rotation_processed += 1
                            obj_processed = True
                            
                        except (ValueError, TypeError) as e:
                            print(f"JsonRotationScaleAdjuster: å¯¹è±¡ {i} çš„rotationå€¼è½¬æ¢å¤±è´¥: {e}")
                    else:
                        print(f"JsonRotationScaleAdjuster: å¯¹è±¡ {i} çš„rotationæ ¼å¼æ— æ•ˆ: {rotation}")
                
                # å¤„ç†scaleå­—æ®µ
                if 'scale' in obj:
                    scale = obj['scale']
                    
                    # æ£€æŸ¥scaleæ˜¯å¦ä¸ºæœ‰æ•ˆçš„åˆ—è¡¨æ ¼å¼
                    if isinstance(scale, list) and len(scale) >= 3:
                        try:
                            # ç¡®ä¿åŸå§‹å€¼æ˜¯æ•°å€¼ç±»å‹
                            original_x = float(scale[0])
                            original_y = float(scale[1]) 
                            original_z = float(scale[2])
                            
                            # è®¡ç®—æ–°çš„ç¼©æ”¾å€¼
                            new_x = original_x * scale_x_multiplier
                            new_y = original_y * scale_y_multiplier
                            new_z = original_z * scale_z_multiplier
                            
                            # æ›´æ–°scaleå€¼
                            obj['scale'][0] = new_x
                            obj['scale'][1] = new_y
                            obj['scale'][2] = new_z
                            
                            scale_processed += 1
                            obj_processed = True
                            
                        except (ValueError, TypeError) as e:
                            print(f"JsonRotationScaleAdjuster: å¯¹è±¡ {i} çš„scaleå€¼è½¬æ¢å¤±è´¥: {e}")
                    else:
                        print(f"JsonRotationScaleAdjuster: å¯¹è±¡ {i} çš„scaleæ ¼å¼æ— æ•ˆ: {scale}")
                
                # å¦‚æœå¯¹è±¡æ²¡æœ‰è¢«å¤„ç†ï¼Œå¢åŠ è·³è¿‡è®¡æ•°
                if not obj_processed:
                    skipped_count += 1
            
            # ç”Ÿæˆå¤„ç†åçš„JSON
            adjusted_json = json.dumps(data, ensure_ascii=False, indent=2)
            
            # è¾“å‡ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯
            print(f"JsonRotationScaleAdjuster å¤„ç†å®Œæˆ:")
            print(f"  â€¢ Rotationå¤„ç†: {rotation_processed} ä¸ªå¯¹è±¡")
            print(f"  â€¢ Scaleå¤„ç†: {scale_processed} ä¸ªå¯¹è±¡")
            print(f"  â€¢ è·³è¿‡å¤„ç†: {skipped_count} ä¸ªå¯¹è±¡")
            print(f"  â€¢ æ—‹è½¬é¡ºåº: {rotation_order}")
            print(f"  â€¢ æ—‹è½¬åç§»: X={rotation_x_offset}Â°, Y={rotation_y_offset}Â°, Z={rotation_z_offset}Â°")
            print(f"  â€¢ ç¼©æ”¾å› å­: X={scale_x_multiplier}, Y={scale_y_multiplier}, Z={scale_z_multiplier}")
            
            return (adjusted_json,)
            
        except json.JSONDecodeError as e:
            error_msg = f"JSONè§£æé”™è¯¯: {str(e)}"
            print(f"JsonRotationScaleAdjuster error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)
        except Exception as e:
            error_msg = f"å¤„ç†æ—‹è½¬å€¼å’Œç¼©æ”¾å€¼æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonRotationScaleAdjuster error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)


class JsonScaleMaxAdjuster:
    """
    JSONå¯¹è±¡é€ä¸ªç¼©æ”¾ç»´åº¦é€‰æ‹©æ€§è°ƒæ•´å™¨
    
    å¯¹JSONæ•°æ®ä¸­å‰Nä¸ªå¯¹è±¡è¿›è¡Œé€ä¸ªå¤„ç†ï¼Œåœ¨æ¯ä¸ªå¯¹è±¡å†…éƒ¨æ‰¾å‡ºscaleçš„æœ€å°ç»´åº¦å¹¶ä¿æŠ¤ï¼Œè°ƒæ•´å…¶ä»–ç»´åº¦ï¼š
    â€¢ é€ä¸ªåˆ†ææ¯ä¸ªå¯¹è±¡çš„scaleå€¼ï¼ˆXã€Yã€Zä¸‰ä¸ªç»´åº¦ï¼‰
    â€¢ åœ¨æ¯ä¸ªå¯¹è±¡å†…éƒ¨æ‰¾å‡ºæœ€å°çš„ç»´åº¦å€¼
    â€¢ åªå¯¹è¯¥å¯¹è±¡å†…å…¶ä»–ä¸¤ä¸ªè¾ƒå¤§çš„ç»´åº¦è¿›è¡Œå¢é‡è°ƒæ•´
    â€¢ ä¿æŒæ¯ä¸ªå¯¹è±¡æœ€å°ç»´åº¦ä¸å˜ï¼Œç»´æŒåŸæœ‰æ•°æ®ç»“æ„
    
    ğŸ¯ åŠŸèƒ½ç‰¹æ€§ï¼š
    â€¢ å¯¹è±¡èŒƒå›´ï¼šå¯è®¾ç½®å¤„ç†å‰å‡ ä¸ªå¯¹è±¡ï¼ˆé»˜è®¤å‰6ä¸ªï¼‰
    â€¢ é€å¯¹è±¡å¤„ç†ï¼šæ¯ä¸ªå¯¹è±¡ç‹¬ç«‹åˆ†æå’Œè°ƒæ•´
    â€¢ ç»´åº¦ä¿æŠ¤ï¼šä¿æŠ¤æ¯ä¸ªå¯¹è±¡çš„æœ€å°ç»´åº¦ä¸è¢«ä¿®æ”¹
    â€¢ å®‰å…¨å¤„ç†ï¼šè‡ªåŠ¨è·³è¿‡æ²¡æœ‰scaleå­—æ®µæˆ–æ ¼å¼é”™è¯¯çš„å¯¹è±¡
    
    âš™ï¸ å‚æ•°è¯´æ˜ï¼š
    â€¢ object_count: å¤„ç†å‰å‡ ä¸ªå¯¹è±¡ï¼ˆé»˜è®¤6ä¸ªï¼‰
    â€¢ max_value_increment: ç»™éæœ€å°ç»´åº¦å¢åŠ çš„æ•°å€¼
    
    ğŸ“ ä½¿ç”¨åœºæ™¯ï¼š
    â€¢ ä¿æŒå¯¹è±¡æœ€ç»†ç»´åº¦ä¸å˜ï¼Œæ‹‰ä¼¸å…¶ä»–ç»´åº¦
    â€¢ åˆ›å»ºéå‡åŒ€ç¼©æ”¾æ•ˆæœ
    â€¢ è°ƒæ•´å¯¹è±¡æ¯”ä¾‹ï¼Œçªå‡ºé•¿å®½ç»´åº¦
    â€¢ ä¿æŠ¤å¯¹è±¡åšåº¦æˆ–é«˜åº¦ç­‰å…³é”®ç»´åº¦
    
    ğŸ’¡ å¤„ç†é€»è¾‘ï¼š
    1. éå†å‰Nä¸ªå¯¹è±¡çš„scaleå­—æ®µ
    2. å¯¹æ¯ä¸ªå¯¹è±¡åˆ†åˆ«å¤„ç†ï¼š
       a. è·å–Xã€Yã€Zä¸‰ä¸ªç»´åº¦å€¼
       b. æ‰¾å‡ºå½“å‰å¯¹è±¡å†…çš„æœ€å°ç»´åº¦å€¼
       c. å¯¹å…¶ä»–ä¸¤ä¸ªç»´åº¦è¿›è¡Œå¢é‡è°ƒæ•´
       d. ä¿æŒæœ€å°ç»´åº¦ä¸å˜
    3. ä¿æŒJSONç»“æ„å’Œå…¶ä»–æ•°æ®ä¸å˜
    
    ğŸ” æ³¨æ„äº‹é¡¹ï¼š
    â€¢ åªå¤„ç†åŒ…å«æœ‰æ•ˆscaleå­—æ®µçš„å¯¹è±¡
    â€¢ scaleå­—æ®µå¿…é¡»æ˜¯é•¿åº¦â‰¥3çš„æ•°ç»„
    â€¢ å¦‚æœå‰Nä¸ªå¯¹è±¡ä¸è¶³ï¼Œåˆ™å¤„ç†å®é™…å­˜åœ¨çš„å¯¹è±¡
    â€¢ å¢é‡å¯ä»¥ä¸ºè´Ÿæ•°ï¼ˆå‡å°‘éæœ€å°ç»´åº¦ï¼‰
    â€¢ å¦‚æœå¯¹è±¡å†…ä¸‰ä¸ªç»´åº¦ç›¸ç­‰ï¼Œåˆ™å…¨éƒ¨è°ƒæ•´
    
    ğŸ“Š å¤„ç†ç¤ºä¾‹ï¼š
    å¯¹è±¡scale=[1.0, 0.5, 0.8], å¢é‡=0.1
    â†’ æœ€å°å€¼0.5ä¿æŒä¸å˜
    â†’ è°ƒæ•´å: [1.1, 0.5, 0.9]
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": "", "tooltip": "åŒ…å«objectsæ•°ç»„çš„JSONæ•°æ®\nå°†å¤„ç†å‰Nä¸ªå¯¹è±¡çš„scaleå€¼\næ‰¾å‡ºæœ€å¤§å€¼å¹¶è¿›è¡Œè°ƒæ•´"})
            },
            "optional": {
                "object_count": (IO.INT, {
                    "default": 6, 
                    "min": 1, 
                    "max": 100,
                    "tooltip": "å¤„ç†å‰å‡ ä¸ªå¯¹è±¡\nâ€¢ é»˜è®¤: 6ä¸ªå¯¹è±¡\nâ€¢ èŒƒå›´: 1åˆ°100\nâ€¢ å¦‚æœå¯¹è±¡ä¸è¶³åˆ™å¤„ç†æ‰€æœ‰å¯ç”¨å¯¹è±¡\nâ€¢ é€ä¸ªå¤„ç†æ¯ä¸ªå¯¹è±¡çš„scaleç»´åº¦"
                }),
                "max_value_increment": (IO.FLOAT, {
                    "default": 0.1, 
                    "min": -1000.0, 
                    "max": 1000.0, 
                    "step": 0.001,
                    "tooltip": "éæœ€å°ç»´åº¦å¢é‡\nâ€¢ æ­£æ•°: å¢åŠ æ¯ä¸ªå¯¹è±¡å†…é™¤æœ€å°ç»´åº¦å¤–çš„å€¼\nâ€¢ è´Ÿæ•°: å‡å°‘æ¯ä¸ªå¯¹è±¡å†…é™¤æœ€å°ç»´åº¦å¤–çš„å€¼\nâ€¢ 0: ä¸è¿›è¡Œè°ƒæ•´\nâ€¢ ç¤ºä¾‹: 0.1è¡¨ç¤ºç»™éæœ€å°ç»´åº¦éƒ½åŠ 0.1\nâ€¢ ç¤ºä¾‹: -0.05è¡¨ç¤ºç»™éæœ€å°ç»´åº¦éƒ½å‡0.05\nâ€¢ æ¯ä¸ªå¯¹è±¡çš„æœ€å°ç»´åº¦å§‹ç»ˆä¿æŒä¸å˜"
                }),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("adjusted_json",)
    FUNCTION = "adjust_scale_max_value"
    CATEGORY = "VVL/json"
    
    def adjust_scale_max_value(self, json_text, object_count=6, max_value_increment=0.1, **kwargs):
        """è°ƒæ•´å‰Nä¸ªå¯¹è±¡ä¸­scaleå€¼çš„æœ€å¤§å€¼"""
        try:
            # è§£æè¾“å…¥JSON
            data = json.loads(json_text)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰objectså­—æ®µ
            if 'objects' not in data or not isinstance(data['objects'], list):
                print("JsonScaleMaxAdjuster: æœªæ‰¾åˆ°æœ‰æ•ˆçš„objectsæ•°ç»„")
                return (json_text,)
            
            objects = data['objects']
            
            # ç¡®å®šå®é™…å¤„ç†çš„å¯¹è±¡æ•°é‡
            actual_count = min(object_count, len(objects))
            if actual_count == 0:
                print("JsonScaleMaxAdjuster: æ²¡æœ‰å¯å¤„ç†çš„å¯¹è±¡")
                return (json_text,)
            
            # é€ä¸ªå¯¹è±¡å¤„ç†scaleå€¼
            valid_objects = 0
            total_adjusted = 0
            total_scale_values = 0
            
            for i in range(actual_count):
                obj = objects[i]
                if not isinstance(obj, dict):
                    continue
                
                if 'scale' in obj:
                    scale = obj['scale']
                    
                    # æ£€æŸ¥scaleæ˜¯å¦ä¸ºæœ‰æ•ˆçš„åˆ—è¡¨æ ¼å¼
                    if isinstance(scale, list) and len(scale) >= 3:
                        try:
                            # è·å–å½“å‰å¯¹è±¡çš„Xã€Yã€Zä¸‰ä¸ªç»´åº¦çš„å€¼
                            x_val = float(scale[0])
                            y_val = float(scale[1])
                            z_val = float(scale[2])
                            
                            # åœ¨å½“å‰å¯¹è±¡å†…æ‰¾å‡ºæœ€å°å€¼
                            min_val_in_obj = min(x_val, y_val, z_val)
                            
                            # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œå¤„ç†ï¼šå¦‚æœä¸æ˜¯æœ€å°å€¼åˆ™è°ƒæ•´
                            obj_adjusted = 0
                            if x_val != min_val_in_obj:
                                scale[0] = x_val + max_value_increment
                                obj_adjusted += 1
                            if y_val != min_val_in_obj:
                                scale[1] = y_val + max_value_increment
                                obj_adjusted += 1
                            if z_val != min_val_in_obj:
                                scale[2] = z_val + max_value_increment
                                obj_adjusted += 1
                            
                            # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœä¸‰ä¸ªå€¼éƒ½ç›¸ç­‰ï¼Œåˆ™å…¨éƒ¨è°ƒæ•´
                            if obj_adjusted == 0 and x_val == y_val == z_val:
                                scale[0] = x_val + max_value_increment
                                scale[1] = y_val + max_value_increment
                                scale[2] = z_val + max_value_increment
                                obj_adjusted = 3
                            
                            total_adjusted += obj_adjusted
                            total_scale_values += 3
                            valid_objects += 1
                            
                            print(f"  å¯¹è±¡[{i}]: scale=[{x_val:.3f}, {y_val:.3f}, {z_val:.3f}], æœ€å°å€¼={min_val_in_obj:.3f}, è°ƒæ•´äº†{obj_adjusted}ä¸ªç»´åº¦")
                            
                        except (ValueError, TypeError) as e:
                            print(f"JsonScaleMaxAdjuster: å¯¹è±¡ {i} çš„scaleå€¼è½¬æ¢å¤±è´¥: {e}")
                    else:
                        print(f"JsonScaleMaxAdjuster: å¯¹è±¡ {i} çš„scaleæ ¼å¼æ— æ•ˆ: {scale}")
            
            if valid_objects == 0:
                print("JsonScaleMaxAdjuster: æœªæ‰¾åˆ°æœ‰æ•ˆçš„scaleå€¼")
                return (json_text,)
            
            # ç”Ÿæˆå¤„ç†åçš„JSON
            adjusted_json = json.dumps(data, ensure_ascii=False, indent=2)
            
            # è¾“å‡ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯
            print(f"JsonScaleMaxAdjuster å¤„ç†å®Œæˆ:")
            print(f"  â€¢ å¤„ç†å¯¹è±¡èŒƒå›´: å‰ {actual_count} ä¸ªå¯¹è±¡")
            print(f"  â€¢ æœ‰æ•ˆå¯¹è±¡æ•°é‡: {valid_objects} ä¸ª")
            print(f"  â€¢ æ€»scaleç»´åº¦: {total_scale_values} ä¸ª")
            print(f"  â€¢ è°ƒæ•´çš„ç»´åº¦æ•°: {total_adjusted} ä¸ª")
            print(f"  â€¢ å¢é‡è°ƒæ•´: +{max_value_increment}")
            print(f"  â€¢ å¤„ç†ç­–ç•¥: æ¯ä¸ªå¯¹è±¡å†…éƒ¨æ’é™¤æœ€å°å€¼ï¼Œè°ƒæ•´å…¶ä»–ç»´åº¦")
            
            return (adjusted_json,)
            
        except json.JSONDecodeError as e:
            error_msg = f"JSONè§£æé”™è¯¯: {str(e)}"
            print(f"JsonScaleMaxAdjuster error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)
        except Exception as e:
            error_msg = f"å¤„ç†scaleæœ€å¤§å€¼æ—¶å‡ºé”™: {str(e)}"
            print(f"JsonScaleMaxAdjuster error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)


class JsonCompressor:
    """
    JSONå‹ç¼©èŠ‚ç‚¹
    
    å°†æ ¼å¼åŒ–çš„JSONå‹ç¼©ä¸ºç´§å‡‘æ ¼å¼ï¼Œç§»é™¤æ‰€æœ‰éå¿…è¦çš„ç©ºç™½å­—ç¬¦ï¼š
    â€¢ ç§»é™¤æ‰€æœ‰ç¼©è¿›ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦
    â€¢ ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦
    â€¢ ç§»é™¤å†’å·å’Œé€—å·åçš„é¢å¤–ç©ºæ ¼
    â€¢ ä¿æŒJSONæ•°æ®ç»“æ„å’Œå†…å®¹å®Œå…¨ä¸å˜
    
    ğŸ¯ åŠŸèƒ½ç‰¹æ€§ï¼š
    â€¢ é«˜æ•ˆå‹ç¼©ï¼šæ˜¾è‘—å‡å°JSONæ–‡ä»¶å¤§å°
    â€¢ å®Œå…¨å…¼å®¹ï¼šä¿æŒJSONè¯­æ³•å’Œæ•°æ®å®Œæ•´æ€§
    â€¢ å®‰å…¨å¤„ç†ï¼šéªŒè¯è¾“å…¥JSONæ ¼å¼æœ‰æ•ˆæ€§
    â€¢ é”™è¯¯å¤„ç†ï¼šæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯åé¦ˆ
    
    ğŸ“Š å‹ç¼©æ•ˆæœï¼š
    â€¢ ç§»é™¤ç¼©è¿›ï¼šèŠ‚çœå¤§é‡ç©ºé—´
    â€¢ ç§»é™¤æ¢è¡Œï¼šå‡å°‘æ–‡ä»¶è¡Œæ•°
    â€¢ ç´§å‡‘åˆ†éš”ç¬¦ï¼šæœ€å°åŒ–è¯­æ³•å­—ç¬¦å ç”¨
    â€¢ ä¿æŒæ•°æ®ï¼šç¡®ä¿æ•°æ®å†…å®¹ä¸ä¸¢å¤±
    
    ğŸ“ ä½¿ç”¨åœºæ™¯ï¼š
    â€¢ å‡å°JSONæ–‡ä»¶ä¼ è¾“å¤§å°
    â€¢ ä¼˜åŒ–å­˜å‚¨ç©ºé—´å ç”¨
    â€¢ æé«˜ç½‘ç»œä¼ è¾“æ•ˆç‡
    â€¢ ç”Ÿæˆç”¨äºAPIçš„ç´§å‡‘JSON
    
    ğŸ’¡ å¤„ç†é€»è¾‘ï¼š
    1. éªŒè¯è¾“å…¥JSONæ ¼å¼æœ‰æ•ˆæ€§
    2. è§£æJSONä¸ºPythonå¯¹è±¡
    3. ä½¿ç”¨ç´§å‡‘æ¨¡å¼é‡æ–°åºåˆ—åŒ–
    4. ç§»é™¤æ‰€æœ‰éå¿…è¦ç©ºç™½å­—ç¬¦
    5. è¿”å›å‹ç¼©åçš„JSONå­—ç¬¦ä¸²
    
    ğŸ” æ³¨æ„äº‹é¡¹ï¼š
    â€¢ å‹ç¼©æ˜¯æ— æŸçš„ï¼Œä¸ä¼šæ”¹å˜æ•°æ®å†…å®¹
    â€¢ å‹ç¼©åçš„JSONä»ç„¶æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
    â€¢ å¯ä»¥é€šè¿‡æ ¼å¼åŒ–å·¥å…·è¿˜åŸä¸ºå¯è¯»æ ¼å¼
    â€¢ å‹ç¼©ç¨‹åº¦å–å†³äºåŸå§‹JSONçš„æ ¼å¼åŒ–ç¨‹åº¦
    
    ğŸ“ˆ å‹ç¼©ç¤ºä¾‹ï¼š
    å‹ç¼©å‰ (228å­—ç¬¦):
    {
      "camera": {
        "position": [50, 0, 0],
        "rotation": [10, 0, 0]
      }
    }
    
    å‹ç¼©å (67å­—ç¬¦):
    {"camera":{"position":[50,0,0],"rotation":[10,0,0]}}
    
    å‹ç¼©ç‡: 70.6%
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": "", "tooltip": "è¦å‹ç¼©çš„JSONæ•°æ®\næ”¯æŒä»»ä½•æœ‰æ•ˆçš„JSONæ ¼å¼\nåŒ…æ‹¬å¸¦ç¼©è¿›ã€æ¢è¡Œçš„æ ¼å¼åŒ–JSON\nå°†è¢«å‹ç¼©ä¸ºç´§å‡‘çš„å•è¡Œæ ¼å¼"})
            },
            "optional": {
                "show_compression_stats": (IO.BOOLEAN, {"default": True, "label_on": "æ˜¾ç¤ºç»Ÿè®¡", "label_off": "éšè—ç»Ÿè®¡", "tooltip": "æ˜¯å¦æ˜¾ç¤ºå‹ç¼©ç»Ÿè®¡ä¿¡æ¯\nâ€¢ å¼€å¯: æ˜¾ç¤ºå‹ç¼©å‰åå­—ç¬¦æ•°å’Œå‹ç¼©ç‡\nâ€¢ å…³é—­: ä»…è¾“å‡ºå‹ç¼©ç»“æœ\nç»Ÿè®¡ä¿¡æ¯ä¼šåœ¨æ§åˆ¶å°ä¸­æ˜¾ç¤º"}),
            },
        }
    
    RETURN_TYPES = (IO.STRING,)
    RETURN_NAMES = ("compressed_json",)
    FUNCTION = "compress_json"
    CATEGORY = "VVL/json"
    
    def compress_json(self, json_text, show_compression_stats=True, **kwargs):
        """å‹ç¼©JSONä¸ºç´§å‡‘æ ¼å¼"""
        try:
            # éªŒè¯å¹¶è§£æè¾“å…¥JSON
            data = json.loads(json_text)
            
            # ä½¿ç”¨ç´§å‡‘æ¨¡å¼åºåˆ—åŒ–JSON
            # separators=(',', ':') ç§»é™¤é€—å·å’Œå†’å·åçš„ç©ºæ ¼
            # ensure_ascii=False ä¿æŒéASCIIå­—ç¬¦
            compressed_json = json.dumps(data, separators=(',', ':'), ensure_ascii=False)
            
            # è®¡ç®—å‹ç¼©ç»Ÿè®¡ä¿¡æ¯
            if show_compression_stats:
                original_size = len(json_text)
                compressed_size = len(compressed_json)
                space_saved = original_size - compressed_size
                compression_ratio = (space_saved / original_size * 100) if original_size > 0 else 0
                
                print(f"JsonCompressor å‹ç¼©å®Œæˆ:")
                print(f"  â€¢ åŸå§‹å¤§å°: {original_size} å­—ç¬¦")
                print(f"  â€¢ å‹ç¼©åå¤§å°: {compressed_size} å­—ç¬¦")
                print(f"  â€¢ èŠ‚çœç©ºé—´: {space_saved} å­—ç¬¦")
                print(f"  â€¢ å‹ç¼©ç‡: {compression_ratio:.1f}%")
                
                # æ˜¾ç¤ºå‹ç¼©æ•ˆæœç¤ºä¾‹ï¼ˆå‰100ä¸ªå­—ç¬¦ï¼‰
                if original_size > 100:
                    print(f"  â€¢ å‹ç¼©å‰é¢„è§ˆ: {json_text[:100]}...")
                    print(f"  â€¢ å‹ç¼©åé¢„è§ˆ: {compressed_json[:100]}...")
            
            return (compressed_json,)
            
        except json.JSONDecodeError as e:
            error_msg = f"JSONæ ¼å¼é”™è¯¯: {str(e)}"
            print(f"JsonCompressor error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)
        except Exception as e:
            error_msg = f"å‹ç¼©JSONæ—¶å‡ºé”™: {str(e)}"
            print(f"JsonCompressor error: {error_msg}")
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)


class DimensionReorderAndScale:
    """
    ä¸‰ç»´æ•°æ®é‡æ–°æ’åºå’Œç¼©æ”¾èŠ‚ç‚¹
    
    å¤„ç†æ ¼å¼å¦‚ [10, 10, 0.3] çš„ä¸‰ç»´æ•°æ®ï¼š
    â€¢ æ”¯æŒä»»æ„è°ƒæ¢é•¿(length)ã€å®½(width)ã€é«˜(height)çš„ä½ç½®
    â€¢ æä¾›ç¼©æ”¾å› å­æ§åˆ¶æ•´ä½“å¤§å°
    â€¢ æ”¯æŒæœ€å°å€¼å’Œæœ€å¤§å€¼é™åˆ¶ï¼ˆæœ€åæ£€éªŒé˜¶æ®µï¼‰
    â€¢ è¾“å‡ºæ—¶é»˜è®¤ç§»é™¤æ–¹æ‹¬å·ï¼Œè¿”å›é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
    
    ğŸ“ è¾“å…¥æ ¼å¼æ”¯æŒï¼š
    â€¢ åˆ—è¡¨: [10, 10, 0.3]
    â€¢ JSONå­—ç¬¦ä¸²: "[10, 10, 0.3]"
    â€¢ é€—å·åˆ†éš”å­—ç¬¦ä¸²: "10, 10, 0.3"
    
    ğŸ”„ æ’åºé€‰é¡¹ï¼š
    â€¢ LWH (é•¿å®½é«˜) - é»˜è®¤é¡ºåº
    â€¢ LHW (é•¿é«˜å®½)
    â€¢ WLH (å®½é•¿é«˜)
    â€¢ WHL (å®½é«˜é•¿)
    â€¢ HLW (é«˜é•¿å®½)
    â€¢ HWL (é«˜å®½é•¿)
    
    âš™ï¸ ç¼©æ”¾æ§åˆ¶ï¼š
    â€¢ scale_factor: å…¨å±€ç¼©æ”¾å› å­ (é»˜è®¤ 1.0)
    â€¢ æ‰€æœ‰ä¸‰ä¸ªç»´åº¦éƒ½ä¼šä¹˜ä»¥æ­¤å› å­
    
    ğŸ”’ æ•°å€¼é™åˆ¶ï¼š
    â€¢ min_value: æœ€å°å€¼é™åˆ¶ (é»˜è®¤ 1.0)
    â€¢ max_value: æœ€å¤§å€¼é™åˆ¶ (é»˜è®¤ 2000.0)
    â€¢ ä»»ä½•å°äºæœ€å°å€¼çš„æ•°å€¼ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºæœ€å°å€¼
    â€¢ ä»»ä½•å¤§äºæœ€å¤§å€¼çš„æ•°å€¼ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºæœ€å¤§å€¼
    â€¢ é™åˆ¶åœ¨æœ€åæ£€éªŒé˜¶æ®µæ‰§è¡Œ
    
    ğŸ“¤ è¾“å‡ºæ ¼å¼ï¼š
    â€¢ é»˜è®¤: "10.0,10.0,0.3" (é€—å·åˆ†éš”ï¼Œæ— ç©ºæ ¼)
    â€¢ å¯é€‰: "10.0, 10.0, 0.3" (ä¿ç•™ç©ºæ ¼)
    â€¢ å¯é€‰: "[10.0,10.0,0.3]" (ä¿ç•™æ–¹æ‹¬å·)
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "dimension_data": (IO.ANY, {"tooltip": "ä¸‰ç»´æ•°æ®è¾“å…¥\næ”¯æŒæ ¼å¼:\nâ€¢ [10, 10, 0.3]\nâ€¢ '[10, 10, 0.3]'\nâ€¢ '10, 10, 0.3'"}),
            },
            "optional": {
                "reorder_pattern": (["LWH", "LHW", "WLH", "WHL", "HLW", "HWL"], {
                    "default": "LWH",
                    "tooltip": "ç»´åº¦é‡æ’æ¨¡å¼\nâ€¢ L=é•¿åº¦(Length)\nâ€¢ W=å®½åº¦(Width) \nâ€¢ H=é«˜åº¦(Height)\n\nä¾‹å¦‚:\nâ€¢ LWH: é•¿-å®½-é«˜ (é»˜è®¤)\nâ€¢ WHL: å®½-é«˜-é•¿\nâ€¢ HLW: é«˜-é•¿-å®½"
                }),
                "scale_factor": (IO.FLOAT, {
                    "default": 1.0, 
                    "min": 0.001, 
                    "max": 1000.0, 
                    "step": 0.01,
                    "tooltip": "å…¨å±€ç¼©æ”¾å› å­\næ‰€æœ‰ç»´åº¦éƒ½ä¹˜ä»¥æ­¤å€¼\nâ€¢ 1.0 = ä¿æŒåŸå°ºå¯¸\nâ€¢ 0.5 = ç¼©å°ä¸€åŠ\nâ€¢ 2.0 = æ”¾å¤§ä¸€å€"
                }),
                "decimal_places": (IO.INT, {
                    "default": 2, 
                    "min": 0, 
                    "max": 6,
                    "tooltip": "å°æ•°ä½æ•°\næ§åˆ¶è¾“å‡ºæ•°å€¼çš„ç²¾åº¦\nâ€¢ 0: æ•´æ•° '10, 10, 0'\nâ€¢ 2: ä¸¤ä½å°æ•° '10.00, 10.00, 0.30'\nâ€¢ 3: ä¸‰ä½å°æ•° '10.000, 10.000, 0.300'"
                }),
                "keep_brackets": (IO.BOOLEAN, {
                    "default": False, 
                    "label_on": "ä¿ç•™[]", 
                    "label_off": "ç§»é™¤[]",
                    "tooltip": "è¾“å‡ºæ ¼å¼æ§åˆ¶\nâ€¢ å…³é—­: '10.0,10.0,0.3'\nâ€¢ å¼€å¯: '[10.0,10.0,0.3]'"
                }),
                "remove_spaces": (IO.BOOLEAN, {
                    "default": False, 
                    "label_on": "ç§»é™¤ç©ºæ ¼", 
                    "label_off": "ä¿ç•™ç©ºæ ¼",
                    "tooltip": "æ˜¯å¦ç§»é™¤è¾“å‡ºä¸­çš„æ‰€æœ‰ç©ºæ ¼\nâ€¢ å¼€å¯: '10.00,10.00,0.30'\nâ€¢ å…³é—­: '10.00, 10.00, 0.30'"
                }),
                "min_value": (IO.FLOAT, {
                    "default": 1.0, 
                    "min": 0.001, 
                    "max": 999999.0, 
                    "step": 0.01,
                    "tooltip": "æœ€å°å€¼é™åˆ¶\nä»»ä½•å°äºæ­¤å€¼çš„ç»´åº¦éƒ½ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºæ­¤å€¼\né»˜è®¤: 1.0"
                }),
                "max_value": (IO.FLOAT, {
                    "default": 2000.0, 
                    "min": 0.001, 
                    "max": 999999.0, 
                    "step": 0.01,
                    "tooltip": "æœ€å¤§å€¼é™åˆ¶\nä»»ä½•å¤§äºæ­¤å€¼çš„ç»´åº¦éƒ½ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºæ­¤å€¼\né»˜è®¤: 2000.0"
                }),
            },
        }
    
    RETURN_TYPES = ("*", "*", "*", "*")
    RETURN_NAMES = ("reordered_dimensions", "dimension_1", "dimension_2", "dimension_3")
    FUNCTION = "reorder_and_scale"
    CATEGORY = "VVL/json"
    
    def _parse_dimension_data(self, dimension_data):
        """è§£æè¾“å…¥çš„ç»´åº¦æ•°æ®ä¸ºä¸‰ä¸ªæ•°å€¼çš„åˆ—è¡¨"""
        if dimension_data is None:
            return None
            
        # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(dimension_data, (list, tuple)) and len(dimension_data) >= 3:
            return [float(dimension_data[0]), float(dimension_data[1]), float(dimension_data[2])]
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        if isinstance(dimension_data, str):
            # ç§»é™¤å¯èƒ½çš„æ–¹æ‹¬å·
            clean_data = dimension_data.strip()
            if clean_data.startswith('[') and clean_data.endswith(']'):
                clean_data = clean_data[1:-1]
            
            # å°è¯•æŒ‰é€—å·åˆ†å‰²
            parts = clean_data.split(',')
            if len(parts) >= 3:
                try:
                    return [float(parts[0].strip()), float(parts[1].strip()), float(parts[2].strip())]
                except ValueError:
                    pass
            
            # å°è¯•ä½œä¸ºJSONè§£æ
            try:
                parsed = json.loads(dimension_data)
                if isinstance(parsed, (list, tuple)) and len(parsed) >= 3:
                    return [float(parsed[0]), float(parsed[1]), float(parsed[2])]
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _reorder_dimensions(self, lwh_values, pattern):
        """æ ¹æ®æŒ‡å®šæ¨¡å¼é‡æ–°æ’åˆ—ç»´åº¦"""
        length, width, height = lwh_values
        
        reorder_map = {
            "LWH": [length, width, height],   # é•¿å®½é«˜ (é»˜è®¤)
            "LHW": [length, height, width],   # é•¿é«˜å®½
            "WLH": [width, length, height],   # å®½é•¿é«˜
            "WHL": [width, height, length],   # å®½é«˜é•¿
            "HLW": [height, length, width],   # é«˜é•¿å®½
            "HWL": [height, width, length],   # é«˜å®½é•¿
        }
        
        return reorder_map.get(pattern, [length, width, height])
    
    def _clamp_values(self, values, min_value, max_value):
        """å°†æ•°å€¼é™åˆ¶åœ¨æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´"""
        clamped_values = []
        for value in values:
            if value < min_value:
                clamped_values.append(min_value)
            elif value > max_value:
                clamped_values.append(max_value)
            else:
                clamped_values.append(value)
        return clamped_values
    
    def reorder_and_scale(self, dimension_data, reorder_pattern="LWH", scale_factor=1.0, keep_brackets=False, decimal_places=2, remove_spaces=True, min_value=1.0, max_value=2000.0, **kwargs):
        """é‡æ–°æ’åºå’Œç¼©æ”¾ä¸‰ç»´æ•°æ®"""
        try:
            # è§£æè¾“å…¥æ•°æ®
            parsed_data = self._parse_dimension_data(dimension_data)
            
            if parsed_data is None:
                error_msg = "æ— æ³•è§£æè¾“å…¥çš„ä¸‰ç»´æ•°æ®ã€‚æœŸæœ›æ ¼å¼: [10, 10, 0.3] æˆ– '10, 10, 0.3'"
                print(f"DimensionReorderAndScale error: {error_msg}")
                return (error_msg, 0.0, 0.0, 0.0)
            
            # åº”ç”¨ç¼©æ”¾å› å­
            scaled_data = [value * scale_factor for value in parsed_data]
            
            # é‡æ–°æ’åˆ—ç»´åº¦
            reordered_data = self._reorder_dimensions(scaled_data, reorder_pattern)
            
            # åº”ç”¨æœ€å°å€¼æœ€å¤§å€¼é™åˆ¶ï¼ˆæœ€åæ£€éªŒï¼‰
            clamped_data = self._clamp_values(reordered_data, min_value, max_value)
            
            # æ ¼å¼åŒ–è¾“å‡ºå’Œå•ç‹¬æ•°å€¼
            if decimal_places == 0:
                # æ•´æ•°æ ¼å¼
                formatted_values = [str(int(round(value))) for value in clamped_data]
                formatted_numbers = [int(round(value)) for value in clamped_data]
            else:
                # å°æ•°æ ¼å¼
                format_str = f"{{:.{decimal_places}f}}"
                formatted_values = [format_str.format(value) for value in clamped_data]
                # å•ç‹¬æ•°å€¼ä¹Ÿåº”ç”¨ç›¸åŒçš„å°æ•°ç²¾åº¦
                formatted_numbers = [round(value, decimal_places) for value in clamped_data]
            
            # ç»„è£…æœ€ç»ˆå­—ç¬¦ä¸²
            separator = "," if remove_spaces else ", "
            if keep_brackets:
                result = f"[{separator.join(formatted_values)}]"
            else:
                result = separator.join(formatted_values)
            
            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            print(f"DimensionReorderAndScale:")
            print(f"  è¾“å…¥: {dimension_data}")
            print(f"  è§£æå: {parsed_data}")
            print(f"  ç¼©æ”¾å› å­: {scale_factor}")
            print(f"  ç¼©æ”¾å: {scaled_data}")
            print(f"  é‡æ’æ¨¡å¼: {reorder_pattern}")
            print(f"  é‡æ’å: {reordered_data}")
            print(f"  æ•°å€¼é™åˆ¶: [{min_value}, {max_value}]")
            print(f"  é™åˆ¶å: {clamped_data}")
            print(f"  æœ€ç»ˆè¾“å‡º: {result}")
            print(f"  å•ç‹¬è¾“å‡º: {formatted_numbers[0]}, {formatted_numbers[1]}, {formatted_numbers[2]}")
            
            return (result, formatted_numbers[0], formatted_numbers[1], formatted_numbers[2])
            
        except Exception as e:
            error_msg = f"å¤„ç†ä¸‰ç»´æ•°æ®æ—¶å‡ºé”™: {str(e)}"
            print(f"DimensionReorderAndScale error: {error_msg}")
            return (error_msg, 0.0, 0.0, 0.0)


class JsonObjectSplitter:
    """
    JSONå¯¹è±¡æ•°ç»„æ¯”ä¾‹æ‹†åˆ†å™¨
    
    å°†JSONæ•°æ®ä¸­çš„objectsæ•°ç»„æŒ‰æ¯”ä¾‹æ‹†åˆ†æˆä¸¤ä»½ï¼š
    â€¢ ä¿æŒå…¶ä»–æ‰€æœ‰å­—æ®µä¸å˜ï¼ˆcameraã€subjectã€sceneç­‰ï¼‰
    â€¢ åªæ‹†åˆ†objectsæ•°ç»„ä¸ºä¸¤éƒ¨åˆ†
    â€¢ æ”¯æŒè‡ªå®šä¹‰æ‹†åˆ†æ¯”ä¾‹ï¼ˆå¦‚ "3:2"ã€"1:1"ï¼‰
    â€¢ é€‚ç”¨äºéœ€è¦åˆ†æ‰¹å¤„ç†åœºæ™¯å¯¹è±¡çš„åœºæ™¯
    
    ğŸ¯ åŠŸèƒ½ç‰¹æ€§ï¼š
    â€¢ æ¯”ä¾‹æ‹†åˆ†ï¼šæŒ‰æŒ‡å®šæ¯”ä¾‹åˆ†é…objects
    â€¢ ç»“æ„ä¿æŒï¼šé™¤objectså¤–çš„æ‰€æœ‰å­—æ®µå®Œå…¨ä¿ç•™
    â€¢ æ™ºèƒ½åˆ†é…ï¼šè‡ªåŠ¨å¤„ç†ä¸èƒ½æ•´é™¤çš„æƒ…å†µ
    â€¢ é”™è¯¯å¤„ç†ï¼šæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯åé¦ˆ
    
    ğŸ“Š æ‹†åˆ†è§„åˆ™ï¼ˆä»¥ "3:2" ä¸ºä¾‹ï¼‰ï¼š
    â€¢ 10ä¸ªå¯¹è±¡ â†’ 6ä¸ª + 4ä¸ªï¼ˆ3:2 = 60%:40%ï¼‰
    â€¢ 5ä¸ªå¯¹è±¡ â†’ 3ä¸ª + 2ä¸ª
    â€¢ 7ä¸ªå¯¹è±¡ â†’ 4ä¸ª + 3ä¸ªï¼ˆå‘ä¸Šå–æ•´ï¼‰
    
    ğŸ“ æ¯”ä¾‹æ ¼å¼ï¼š
    â€¢ "1:1" - å¹³å‡æ‹†åˆ†ï¼ˆé»˜è®¤ï¼‰
    â€¢ "3:2" - 60% vs 40%
    â€¢ "2:1" - 66.7% vs 33.3%
    â€¢ "4:1" - 80% vs 20%
    
    ğŸ“ ä½¿ç”¨åœºæ™¯ï¼š
    â€¢ åˆ†æ‰¹å¤„ç†å¤§é‡åœºæ™¯å¯¹è±¡
    â€¢ å°†åœºæ™¯æ‹†åˆ†ä¸ºå‰æ™¯å’ŒèƒŒæ™¯
    â€¢ æŒ‰é‡è¦æ€§åˆ†ç»„ç®¡ç†å¯¹è±¡
    â€¢ ä¼˜åŒ–æ¸²æŸ“å’ŒåŠ è½½æ€§èƒ½
    
    ğŸ’¡ å¤„ç†é€»è¾‘ï¼š
    1. è§£æè¾“å…¥çš„JSONæ•°æ®å’Œæ¯”ä¾‹å‚æ•°
    2. æå–objectsæ•°ç»„
    3. æ ¹æ®æ¯”ä¾‹è®¡ç®—åˆ†å‰²ç‚¹
    4. åˆ›å»ºä¸¤ä»½å®Œæ•´çš„JSONï¼Œåˆ†åˆ«åŒ…å«ç›¸åº”æ¯”ä¾‹çš„objects
    5. ä¿æŒå…¶ä»–æ‰€æœ‰å­—æ®µä¸å˜
    
    ğŸ” æ³¨æ„äº‹é¡¹ï¼š
    â€¢ è¾“å…¥JSONå¿…é¡»åŒ…å«objectså­—æ®µ
    â€¢ objectså¿…é¡»æ˜¯æ•°ç»„ç±»å‹
    â€¢ å¦‚æœobjectsä¸ºç©ºï¼Œä¸¤ä¸ªè¾“å‡ºç›¸åŒ
    â€¢ æ‹†åˆ†ä¿æŒåŸå§‹é¡ºåºä¸å˜
    â€¢ æ¯”ä¾‹æ ¼å¼é”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤çš„ 1:1
    
    ğŸ“ˆ æ‹†åˆ†ç¤ºä¾‹ï¼š
    è¾“å…¥ï¼š10ä¸ªobjectsï¼Œæ¯”ä¾‹ "3:2"
    è¾“å‡º1ï¼šå‰6ä¸ªobjectsï¼ˆ60%ï¼‰
    è¾“å‡º2ï¼šå4ä¸ªobjectsï¼ˆ40%ï¼‰
    
    è¾“å…¥ï¼š5ä¸ªobjectsï¼Œæ¯”ä¾‹ "1:1"
    è¾“å‡º1ï¼šå‰3ä¸ªobjects
    è¾“å‡º2ï¼šå2ä¸ªobjects
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "json_text": (IO.STRING, {"multiline": True, "default": "", "tooltip": "åŒ…å«objectsæ•°ç»„çš„JSONæ•°æ®\nå°†æŒ‰æ¯”ä¾‹æ‹†åˆ†objectsæ•°ç»„\nå…¶ä»–å­—æ®µï¼ˆcameraã€subjectç­‰ï¼‰ä¿æŒä¸å˜"})
            },
            "optional": {
                "ratio": (IO.STRING, {"default": "1:1", "tooltip": "æ‹†åˆ†æ¯”ä¾‹\næ ¼å¼: \"æ•°å­—:æ•°å­—\"\nâ€¢ \"1:1\" - å¹³å‡æ‹†åˆ†ï¼ˆé»˜è®¤ï¼‰\nâ€¢ \"3:2\" - 60% vs 40%\nâ€¢ \"2:1\" - 66.7% vs 33.3%\nâ€¢ \"4:1\" - 80% vs 20%\nç¬¬ä¸€ä¸ªæ•°å­—å¯¹åº”ç¬¬ä¸€ä»½è¾“å‡º"})
            }
        }
    
    RETURN_TYPES = (IO.STRING, IO.STRING, IO.INT, IO.INT)
    RETURN_NAMES = ("json_part1", "json_part2", "part1_count", "part2_count")
    FUNCTION = "split_objects"
    CATEGORY = "VVL/json"
    
    def split_objects(self, json_text, ratio="1:1", **kwargs):
        """å°†JSONä¸­çš„objectsæ•°ç»„æŒ‰æ¯”ä¾‹æ‹†åˆ†æˆä¸¤ä»½"""
        try:
            # è§£æè¾“å…¥JSON
            data = json.loads(json_text)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰objectså­—æ®µ
            if 'objects' not in data:
                error_msg = "æœªæ‰¾åˆ°objectså­—æ®µ"
                print(f"JsonObjectSplitter error: {error_msg}")
                return (
                    json.dumps({"error": error_msg}, ensure_ascii=False), 
                    json.dumps({"error": error_msg}, ensure_ascii=False),
                    0,
                    0
                )
            
            if not isinstance(data['objects'], list):
                error_msg = "objectså­—æ®µä¸æ˜¯æ•°ç»„ç±»å‹"
                print(f"JsonObjectSplitter error: {error_msg}")
                return (
                    json.dumps({"error": error_msg}, ensure_ascii=False),
                    json.dumps({"error": error_msg}, ensure_ascii=False),
                    0,
                    0
                )
            
            objects = data['objects']
            total_count = len(objects)
            
            # å¦‚æœæ²¡æœ‰å¯¹è±¡ï¼Œè¿”å›ä¸¤ä¸ªç›¸åŒçš„JSON
            if total_count == 0:
                print("JsonObjectSplitter: objectsæ•°ç»„ä¸ºç©º")
                return (json_text, json_text, 0, 0)
            
            # è§£ææ¯”ä¾‹å‚æ•°
            ratio_parts = [1, 1]  # é»˜è®¤ 1:1
            try:
                ratio_str = str(ratio).strip()
                if ':' in ratio_str:
                    parts = ratio_str.split(':')
                    if len(parts) == 2:
                        ratio_parts[0] = float(parts[0].strip())
                        ratio_parts[1] = float(parts[1].strip())
                        
                        # éªŒè¯æ¯”ä¾‹å€¼æ˜¯å¦æœ‰æ•ˆ
                        if ratio_parts[0] <= 0 or ratio_parts[1] <= 0:
                            print(f"JsonObjectSplitter: æ¯”ä¾‹å€¼æ— æ•ˆ ({ratio_str})ï¼Œä½¿ç”¨é»˜è®¤ 1:1")
                            ratio_parts = [1, 1]
                    else:
                        print(f"JsonObjectSplitter: æ¯”ä¾‹æ ¼å¼é”™è¯¯ ({ratio_str})ï¼Œä½¿ç”¨é»˜è®¤ 1:1")
                else:
                    print(f"JsonObjectSplitter: æ¯”ä¾‹æ ¼å¼é”™è¯¯ ({ratio_str})ï¼Œä½¿ç”¨é»˜è®¤ 1:1")
            except Exception as e:
                print(f"JsonObjectSplitter: è§£ææ¯”ä¾‹æ—¶å‡ºé”™ ({ratio}): {e}ï¼Œä½¿ç”¨é»˜è®¤ 1:1")
                ratio_parts = [1, 1]
            
            # è®¡ç®—åˆ†å‰²ç‚¹
            # æ ¹æ®æ¯”ä¾‹è®¡ç®—ç¬¬ä¸€ä»½åº”è¯¥åŒ…å«çš„å¯¹è±¡æ•°
            ratio_sum = ratio_parts[0] + ratio_parts[1]
            part1_ratio = ratio_parts[0] / ratio_sum
            split_point = int(total_count * part1_ratio + 0.5)  # å››èˆäº”å…¥
            
            # ç¡®ä¿åˆ†å‰²ç‚¹åœ¨æœ‰æ•ˆèŒƒå›´å†…
            split_point = max(0, min(split_point, total_count))
            
            # åˆ›å»ºä¸¤ä»½æ•°æ®ï¼ˆä½¿ç”¨æ·±æ‹·è´é¿å…å¼•ç”¨é—®é¢˜ï¼‰
            data1 = copy.deepcopy(data)
            data1['objects'] = objects[:split_point]
            
            data2 = copy.deepcopy(data)
            data2['objects'] = objects[split_point:]
            
            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            json_part1 = json.dumps(data1, ensure_ascii=False, indent=2)
            json_part2 = json.dumps(data2, ensure_ascii=False, indent=2)
            
            # è·å–æ‹†åˆ†åçš„æ•°é‡
            part1_count = len(data1['objects'])
            part2_count = len(data2['objects'])
            
            # è¾“å‡ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯
            part1_percent = (part1_count / total_count * 100) if total_count > 0 else 0
            part2_percent = (part2_count / total_count * 100) if total_count > 0 else 0
            
            print(f"JsonObjectSplitter æ‹†åˆ†å®Œæˆ:")
            print(f"  â€¢ è¾“å…¥æ€»å¯¹è±¡æ•°: {total_count}")
            print(f"  â€¢ è®¾ç½®æ¯”ä¾‹: {ratio_parts[0]}:{ratio_parts[1]} ({ratio})")
            print(f"  â€¢ ç¬¬ä¸€ä»½å¯¹è±¡æ•°: {part1_count} ({part1_percent:.1f}%)")
            print(f"  â€¢ ç¬¬äºŒä»½å¯¹è±¡æ•°: {part2_count} ({part2_percent:.1f}%)")
            print(f"  â€¢ åˆ†å‰²ç‚¹ç´¢å¼•: {split_point}")
            
            # æ˜¾ç¤ºæ‹†åˆ†è¯¦æƒ…
            if total_count <= 10:
                print(f"  â€¢ ç¬¬ä¸€ä»½å¯¹è±¡: {[obj.get('name', f'å¯¹è±¡{i}') for i, obj in enumerate(data1['objects'])]}")
                print(f"  â€¢ ç¬¬äºŒä»½å¯¹è±¡: {[obj.get('name', f'å¯¹è±¡{i}') for i, obj in enumerate(data2['objects'])]}")
            
            return (json_part1, json_part2, part1_count, part2_count)
            
        except json.JSONDecodeError as e:
            error_msg = f"JSONè§£æé”™è¯¯: {str(e)}"
            print(f"JsonObjectSplitter error: {error_msg}")
            return (
                json.dumps({"error": error_msg}, ensure_ascii=False),
                json.dumps({"error": error_msg}, ensure_ascii=False),
                0,
                0
            )
        except Exception as e:
            error_msg = f"æ‹†åˆ†JSONæ—¶å‡ºé”™: {str(e)}"
            print(f"JsonObjectSplitter error: {error_msg}")
            import traceback
            traceback.print_exc()
            return (
                json.dumps({"error": error_msg}, ensure_ascii=False),
                json.dumps({"error": error_msg}, ensure_ascii=False),
                0,
                0
            )


class IndexOffsetAdjuster:
    """
    ç´¢å¼•åç§»è°ƒæ•´å™¨
    
    è°ƒæ•´å­—ç¬¦ä¸²åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ çš„ç´¢å¼•ç¼–å·ï¼š
    â€¢ å¤„ç†æ ¼å¼å¦‚ "0,value" çš„å­—ç¬¦ä¸²åˆ—è¡¨
    â€¢ ç»™æ¯ä¸ªå…ƒç´ çš„ç¬¬ä¸€ä¸ªæ•°å­—ï¼ˆç´¢å¼•ï¼‰æ·»åŠ åç§»é‡
    â€¢ ä¿æŒé€—å·åçš„å€¼éƒ¨åˆ†ä¸å˜
    â€¢ é€‚ç”¨äºéœ€è¦é‡æ–°ç¼–å·æˆ–åˆå¹¶å¤šä¸ªåˆ—è¡¨çš„åœºæ™¯
    
    ğŸ¯ åŠŸèƒ½ç‰¹æ€§ï¼š
    â€¢ ç´¢å¼•åç§»ï¼šç»™æ‰€æœ‰ç´¢å¼•æ·»åŠ å›ºå®šåç§»é‡
    â€¢ æ ¼å¼ä¿æŒï¼šä¿æŒåŸå§‹å­—ç¬¦ä¸²æ ¼å¼
    â€¢ æ‰¹é‡å¤„ç†ï¼šä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåˆ—è¡¨
    â€¢ çµæ´»è¾“å…¥ï¼šæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼
    
    ğŸ“Š å¤„ç†ç¤ºä¾‹ï¼ˆoffset=10ï¼‰ï¼š
    è¾“å…¥ï¼š["0,url1", "1,url2", "2,url3"]
    è¾“å‡ºï¼š["10,url1", "11,url2", "12,url3"]
    
    è¾“å…¥ï¼š["5,data_a", "6,data_b", "7,data_c"]
    è¾“å‡ºï¼š["15,data_a", "16,data_b", "17,data_c"]
    
    ğŸ“ ä½¿ç”¨åœºæ™¯ï¼š
    â€¢ åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„æ•°æ®æ—¶é‡æ–°ç¼–å·
    â€¢ åœ¨å¾ªç¯ä¸­ä¸ºæ¯ä¸ªæ‰¹æ¬¡è®¾ç½®ä¸åŒçš„èµ·å§‹ç´¢å¼•
    â€¢ å°†å¤šä¸ªæ•°æ®æºçš„ç´¢å¼•ç»Ÿä¸€åˆ°åŒä¸€å‘½åç©ºé—´
    â€¢ é¿å…ä¸åŒæ‰¹æ¬¡ä¹‹é—´çš„ç´¢å¼•å†²çª
    
    ğŸ’¡ å¤„ç†é€»è¾‘ï¼š
    1. æ¥æ”¶è¾“å…¥åˆ—è¡¨å’Œåç§»é‡
    2. éå†æ¯ä¸ªå…ƒç´ 
    3. æå–é€—å·å‰çš„ç´¢å¼•æ•°å­—
    4. ç»™ç´¢å¼•åŠ ä¸Šåç§»é‡
    5. ä¿æŒé€—å·åçš„å†…å®¹ä¸å˜
    6. é‡æ–°ç»„åˆå­—ç¬¦ä¸²
    
    ğŸ” æ³¨æ„äº‹é¡¹ï¼š
    â€¢ è¾“å…¥åˆ—è¡¨ä¸­çš„å…ƒç´ åº”è¯¥åŒ…å«é€—å·åˆ†éš”ç¬¦
    â€¢ é€—å·å‰çš„éƒ¨åˆ†åº”è¯¥æ˜¯æ•°å­—æˆ–å¯è½¬æ¢ä¸ºæ•°å­—çš„å­—ç¬¦ä¸²
    â€¢ å¦‚æœæŸä¸ªå…ƒç´ æ ¼å¼ä¸æ­£ç¡®ï¼Œä¼šè·³è¿‡å¹¶ä¿æŒåŸæ ·
    â€¢ åç§»é‡å¯ä»¥æ˜¯è´Ÿæ•°ï¼ˆä½†è¦æ³¨æ„ç»“æœç´¢å¼•ä¸è¦å°äº0ï¼‰
    
    ğŸ“ˆ å·¥ä½œæµç¤ºä¾‹ï¼š
    ç¬¬ä¸€æ‰¹ï¼š["0,url_a", "1,url_b"] â†’ å¾ªç¯å¤„ç†
    ç¬¬äºŒæ‰¹ï¼š["0,url_c", "1,url_d"] â†’ offset=2 â†’ ["2,url_c", "3,url_d"] â†’ å¾ªç¯å¤„ç†
    è¿™æ ·å¯ä»¥é¿å…ä¸¤æ‰¹æ•°æ®çš„ç´¢å¼•å†²çª
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "input_list": (IO.ANY, {"tooltip": "è¾“å…¥çš„åˆ—è¡¨æ•°æ®\næ ¼å¼: [\"ç´¢å¼•,å€¼\", ...]\nä¾‹å¦‚: [\"0,url1\", \"1,url2\", \"2,url3\"]"}),
                "offset": (IO.INT, {"default": 0, "min": -99999, "max": 99999, "step": 1, "tooltip": "ç´¢å¼•åç§»é‡\nå°†æ·»åŠ åˆ°æ¯ä¸ªå…ƒç´ çš„ç´¢å¼•ä¸Š\nâ€¢ æ­£æ•°: å¢åŠ ç´¢å¼•å€¼\nâ€¢ è´Ÿæ•°: å‡å°‘ç´¢å¼•å€¼\nâ€¢ 0: ä¿æŒä¸å˜"})
            }
        }
    
    RETURN_TYPES = (IO.ANY,)
    RETURN_NAMES = ("adjusted_list",)
    FUNCTION = "adjust_index_offset"
    CATEGORY = "VVL/json"
    
    def adjust_index_offset(self, input_list, offset, **kwargs):
        """è°ƒæ•´åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ çš„ç´¢å¼•åç§»é‡"""
        try:
            # å¤„ç†è¾“å…¥æ•°æ®
            if input_list is None:
                print("IndexOffsetAdjuster: è¾“å…¥ä¸ºç©º")
                return ([],)
            
            # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºåˆ—è¡¨
            if isinstance(input_list, str):
                try:
                    import json
                    input_list = json.loads(input_list)
                except Exception:
                    print("IndexOffsetAdjuster: æ— æ³•å°†å­—ç¬¦ä¸²è§£æä¸ºåˆ—è¡¨")
                    return ([],)
            
            # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨ç±»å‹
            if not isinstance(input_list, (list, tuple)):
                print(f"IndexOffsetAdjuster: è¾“å…¥ä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œç±»å‹={type(input_list)}")
                return ([input_list],)
            
            # å¤„ç†åˆ—è¡¨
            adjusted_list = []
            processed_count = 0
            skipped_count = 0
            
            for i, item in enumerate(input_list):
                try:
                    # ç¡®ä¿å…ƒç´ æ˜¯å­—ç¬¦ä¸²
                    item_str = str(item) if not isinstance(item, str) else item
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é€—å·
                    if ',' not in item_str:
                        # æ²¡æœ‰é€—å·ï¼Œä¿æŒåŸæ ·
                        adjusted_list.append(item)
                        skipped_count += 1
                        continue
                    
                    # åˆ†å‰²å­—ç¬¦ä¸²
                    parts = item_str.split(',', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªé€—å·
                    
                    if len(parts) != 2:
                        # æ ¼å¼ä¸æ­£ç¡®ï¼Œä¿æŒåŸæ ·
                        adjusted_list.append(item)
                        skipped_count += 1
                        continue
                    
                    # æå–ç´¢å¼•å’Œå€¼éƒ¨åˆ†
                    index_str = parts[0].strip()
                    value_part = parts[1]
                    
                    # å°è¯•å°†ç´¢å¼•è½¬æ¢ä¸ºæ•°å­—
                    try:
                        original_index = int(index_str)
                        new_index = original_index + offset
                        
                        # é‡æ–°ç»„åˆå­—ç¬¦ä¸²
                        adjusted_item = f"{new_index},{value_part}"
                        adjusted_list.append(adjusted_item)
                        processed_count += 1
                        
                    except ValueError:
                        # ç´¢å¼•éƒ¨åˆ†ä¸æ˜¯æ•°å­—ï¼Œä¿æŒåŸæ ·
                        adjusted_list.append(item)
                        skipped_count += 1
                        
                except Exception as e:
                    # å¤„ç†å•ä¸ªå…ƒç´ æ—¶å‡ºé”™ï¼Œä¿æŒåŸæ ·
                    print(f"IndexOffsetAdjuster: å¤„ç†å…ƒç´  {i} æ—¶å‡ºé”™: {e}")
                    adjusted_list.append(item)
                    skipped_count += 1
            
            # è¾“å‡ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯
            print(f"IndexOffsetAdjuster å¤„ç†å®Œæˆ:")
            print(f"  â€¢ è¾“å…¥å…ƒç´ æ•°: {len(input_list)}")
            print(f"  â€¢ åç§»é‡: {offset}")
            print(f"  â€¢ æˆåŠŸå¤„ç†: {processed_count} ä¸ª")
            print(f"  â€¢ è·³è¿‡/ä¿æŒ: {skipped_count} ä¸ª")
            
            # æ˜¾ç¤ºå¤„ç†ç¤ºä¾‹ï¼ˆå‰3ä¸ªå…ƒç´ ï¼‰
            if processed_count > 0 and len(input_list) <= 10:
                print(f"  â€¢ å¤„ç†ç¤ºä¾‹:")
                for i in range(min(3, len(input_list))):
                    if i < len(adjusted_list):
                        original = str(input_list[i]) if i < len(input_list) else ""
                        adjusted = str(adjusted_list[i])
                        if original != adjusted:
                            print(f"    [{i}] {original} â†’ {adjusted}")
            
            return (adjusted_list,)
            
        except Exception as e:
            error_msg = f"è°ƒæ•´ç´¢å¼•åç§»é‡æ—¶å‡ºé”™: {str(e)}"
            print(f"IndexOffsetAdjuster error: {error_msg}")
            import traceback
            traceback.print_exc()
            return ([],)


# Node class mappings
NODE_CLASS_MAPPINGS = {
    "JsonObjectDeduplicator": JsonObjectDeduplicator,
    "JsonObjectMerger": JsonObjectMerger,
    "JsonExtractSubjectNamesScales": JsonExtractSubjectNamesScales,
    "ApplyUrlsToJson": ApplyUrlsToJson,
    "JsonMarkdownCleaner": JsonMarkdownCleaner,
    "IndexUrlPairDeduplicator": IndexUrlPairDeduplicator,
    "JsonArrayElementFieldExtractor": JsonArrayElementFieldExtractor,
    "JsonRotationScaleAdjuster": JsonRotationScaleAdjuster,
    "JsonScaleMaxAdjuster": JsonScaleMaxAdjuster,
    "JsonCompressor": JsonCompressor,
    "DimensionReorderAndScale": DimensionReorderAndScale,
    "JsonObjectSplitter": JsonObjectSplitter,
    "IndexOffsetAdjuster": IndexOffsetAdjuster
}

# Node display name mappings
NODE_DISPLAY_NAME_MAPPINGS = {
    "JsonObjectDeduplicator": "VVL JSON Object Deduplicator",
    "JsonObjectMerger": "VVL JSON Object Merger",
    "JsonExtractSubjectNamesScales": "VVL JSON Extract: subject, names, scales",
    "ApplyUrlsToJson": "VVL Apply URLs to JSON",
    "JsonMarkdownCleaner": "VVL JSON Markdown Cleaner",
    "IndexUrlPairDeduplicator": "VVL Index-URL Pair Deduplicator",
    "JsonArrayElementFieldExtractor": "VVL JSON Array Element Field Extractor",
    "JsonRotationScaleAdjuster": "VVL JSON Rotation & Scale Adjuster",
    "JsonScaleMaxAdjuster": "VVL JSON Scale Max Value Adjuster",
    "JsonCompressor": "VVL JSON Compressor",
    "DimensionReorderAndScale": "VVL Dimension Reorder and Scale",
    "JsonObjectSplitter": "VVL JSON Object Splitter",
    "IndexOffsetAdjuster": "VVL Index Offset Adjuster",
}